{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variáveis globais e funções para pré-processamento do dataset:\n",
    "JPGs exportados fullsize (4000x4000 ou 3264x3264) sRGB via Ligthroom com EXIF recuperado do RAW original, exceto orientação. Comando abaixo é usado para substituir o EXIF na exportação (requer LR plugin):\n",
    "```\n",
    "C:\\PROGRA~1\\ImageGlass\\exiftool -tagsFromFile \"{FullMasterFile}\" -all:all -x Orientation \"{FullExportedFile}\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import json\n",
    "import torch\n",
    "from torchvision import models\n",
    "import torch.nn as nn\n",
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "import datetime\n",
    "\n",
    "subject_distance_min = 0.05\n",
    "subject_distance_max = 50.0\n",
    "focal_lenght_min = 16.0\n",
    "focal_lenght_max = 640.0\n",
    "fstop_min = 1.2\n",
    "fstop_max = 48.0\n",
    "\n",
    "preprocessing_worker_count = 15 # for image tensor generation\n",
    "plotting_worker_count = 4       # for image tensor metadata reading. Doesn't scale beyond that on E5-2696v3 for some reason. Probably because of the GIL (as per Codestral). Who's GIL? \n",
    "\n",
    "resnet_version = 50  # User can set this to 18, 34, 50, 101, or 152\n",
    "\n",
    "# Dictionary to map resnet_version to the corresponding ResNet model and weights\n",
    "resnet_models = {\n",
    "    18: (models.resnet18, models.ResNet18_Weights.IMAGENET1K_V1),\n",
    "    34: (models.resnet34, models.ResNet34_Weights.IMAGENET1K_V1),\n",
    "    50: (models.resnet50, models.ResNet50_Weights.IMAGENET1K_V2),\n",
    "    101: (models.resnet101, models.ResNet101_Weights.IMAGENET1K_V1),\n",
    "    152: (models.resnet152, models.ResNet152_Weights.IMAGENET1K_V1)\n",
    "}\n",
    "\n",
    "# Select the ResNet model based on the resnet_version\n",
    "selected_resnet_model, selected_resnet_weights = resnet_models[resnet_version]\n",
    "selected_resnet = selected_resnet_model(weights=selected_resnet_weights)\n",
    "\n",
    "# Hyperparameters\n",
    "max_epochs = 250        # limite de épocas. Será interrompido antes se não houver melhoria no desempenho do modelo após um número específico de épocas (patience).\n",
    "learning_rate = 0.001   # taxa de aprendizado inicial para o otimizador Adam. A taxa de aprendizado controla o tamanho dos passos que o otimizador dá durante o treinamento. Um valor maior pode fazer com que o modelo converja mais rapidamente, mas também pode fazer com que ele oscile ou não converja. Um valor muito pequeno pode levar a um treinamento muito lento ou a um modelo que não converge.\n",
    "patience = 10           # Número de épocas sem melhoria antes de parar o treinamento precocemente (early stopping)\n",
    "unfreeze_epoch = 20     # Época em que as camadas congeladas do modelo começam a ser descongeladas para permitir ajustes finos dos pesos durante o treinamento.\n",
    "\n",
    "#image size and batch size select:\n",
    "final_dimensions = 896      #input dimension per side, multiple of 224 as recommended for ResNet. 672 is the largest multiple of 224 that fits in GPU memory with batch size 128\n",
    "dataloader_batch_size = 16  # até 56 de 672px para ResNet50 em placa com 24GB livres. 88 de 896px para ResNet34 em placa com 24GB livres. 24 para ResNet50 com 3090 em uso e 896px.\n",
    "\n",
    "augmentation_params = [         # Array de parâmetros para criação de dados aumentados. Formato: (crop factor, rotation angle)\n",
    "    (1, 0), (1.0625, 0), (1.125, 0), (1.175, 0),\n",
    "    (1.25, -6), (1.25, 0), (1.25, 6),\n",
    "    (1.3333, 0),\n",
    "    (1.5, -18), (1.5, -9), (1.5, 0), (1.5, 9), (1.5, 18),\n",
    "    (2, 0),\n",
    "    (2.5, -36), (2.5, -18), (2.5, -9), (2.5, 9), (2.5, 18), (2.5, 36),\n",
    "    (3, 0)\n",
    "]\n",
    "\n",
    "\n",
    "#\n",
    "# Disponibilidade e escolha de GPU:\n",
    "# ---------------------------------\n",
    "#\n",
    "\n",
    "gpu_index = 0  # Palit RTX 3090: 0 (display, menos VRAM livre, possivelmente mais rápida); Gigabyte RTX 3090 Blower (mais VRAM livre, menos dissipação de calor, mais ruído): 1\n",
    "\n",
    "cuda_available = torch.cuda.is_available()\n",
    "print(f\"CUDA disponível: {cuda_available}\")\n",
    "num_gpus = torch.cuda.device_count()\n",
    "print(f\"Quantidade de GPUs disponíveis: {num_gpus}\")\n",
    "torch.cuda.set_device(gpu_index)\n",
    "\n",
    "current_device = torch.cuda.current_device()\n",
    "print(f\"GPU atual em uso: ID {current_device}\")\n",
    "\n",
    "if cuda_available:\n",
    "    device = torch.device(\"cuda\")\n",
    "    device_name = torch.cuda.get_device_name(device)\n",
    "    print(f\"\\tNome dispositivo CUDA:\\t{device_name}\")\n",
    "    \n",
    "    cuda_version = torch.version.cuda\n",
    "    compute_capability = torch.cuda.get_device_capability(device)\n",
    "    tensor_core_supported = compute_capability[0] >= 7\n",
    "    print(f\"\\tVersão CUDA: {cuda_version} \\tCompute Capability: {compute_capability}\\t Tensor Core Support: {tensor_core_supported}\\n\")\n",
    "    if(tensor_core_supported):\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        torch.backends.cudnn.allow_tf32 = True\n",
    "        print(f\"\\tTF32 ativado para matmul e cudnn.\\n\")\n",
    "\n",
    "#\n",
    "# Caminhos de diretório: \n",
    "# ---------------------- \n",
    "#\n",
    "\n",
    "tensor_directory = \"D:\\\\TEMTC-CN\\\\tensors\"\n",
    "checkpoint_dir = f\"D:\\\\TEMTC-CN\\\\checkpoints\\\\resnet{resnet_version}\"\n",
    "split_indices_path = f\"D:\\\\TEMTC-CN\\\\split_indices.pkl\"\n",
    "image_directory = \"D:\\\\TEMTC-CN\\\\4000px 1k\"\n",
    "# image_directory = \"D:\\\\TEMTC-CN\\\\4000px test\" \n",
    "\n",
    "image_files = [os.path.join(image_directory, f) for f in os.listdir(image_directory) if f.endswith(\".jpg\")]\n",
    "\n",
    "\n",
    "#\n",
    "# Funções requeridas para o treinamento e teste do modelo\n",
    "# --------------------------------------------------------\n",
    "#\n",
    "\n",
    "# Normalization and denormalization functions\n",
    "def normalize_fstop(value, min_value, max_value):\n",
    "    return (1 - (value - min_value) / (max_value - min_value)) ** 2\n",
    "\n",
    "def normalize_focal_length(value, min_value, max_value):\n",
    "    return (value - min_value) / (max_value - min_value)\n",
    "\n",
    "def normalize_subject_distance(value, min_value, max_value):\n",
    "    if value < min_value:\n",
    "        return 0.0\n",
    "    elif value > max_value:\n",
    "        return 1.0\n",
    "    else:\n",
    "        return (value - min_value) / (max_value - min_value)\n",
    "\n",
    "def denormalize_fstop(value, min_value, max_value):\n",
    "    return max_value - (value ** 0.5 * (max_value - min_value))\n",
    "\n",
    "def denormalize_focal_length(value, min_value, max_value):\n",
    "    return value * (max_value - min_value) + min_value\n",
    "\n",
    "def denormalize_subject_distance(value, min_value, max_value):\n",
    "    if value == 1.0:\n",
    "        return float('inf')\n",
    "    else:\n",
    "        return value * (max_value - min_value) + min_value\n",
    "\n",
    "def extract_exif_data(img_path):\n",
    "    result = subprocess.run(['C:\\\\PROGRA~1\\\\ImageGlass\\\\exiftool', '-json', img_path], stdout=subprocess.PIPE)\n",
    "    exif_data = json.loads(result.stdout)[0]\n",
    "    focal_length = exif_data.get('FocalLength', 0)\n",
    "    f_stop = exif_data.get('FNumber', 0)\n",
    "    subject_distance = exif_data.get('FocusDistance2', 0)\n",
    "    scale_factor = exif_data.get('ScaleFactorTo35mmEquivalent', 1.5)  # Default value if not available\n",
    "\n",
    "    focal_length = float(focal_length.split()[0]) if isinstance(focal_length, str) else float(focal_length)\n",
    "    f_stop = float(f_stop.split()[0]) if isinstance(f_stop, str) else float(f_stop)\n",
    "    subject_distance = float(subject_distance.split()[0]) if isinstance(subject_distance, str) else float(subject_distance)\n",
    "    scale_factor = float(scale_factor.split()[0]) if isinstance(scale_factor, str) else float(scale_factor)\n",
    "\n",
    "    # Scale focal length and f/stop using the scale factor\n",
    "    focal_length_scaled = focal_length * scale_factor\n",
    "    f_stop_scaled = f_stop * scale_factor\n",
    "    # do not scale subject distance\n",
    "\n",
    "    return torch.tensor([focal_length_scaled, f_stop_scaled, subject_distance])\n",
    "\n",
    "# Function to calculate MAPE and MSLE:\n",
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    return torch.mean(torch.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "def mean_squared_logarithmic_error(y_true, y_pred):\n",
    "    return torch.mean((torch.log1p(y_true) - torch.log1p(y_pred)) ** 2)\n",
    "\n",
    "# Function to calculate percentage error\n",
    "def calculate_percentage_error(real, inferred):\n",
    "    return np.abs((real - inferred) / real) * 100\n",
    "\n",
    "# Custom dataset class for loading tensors. Qwen32B que disse pra fazer assim, parece funcionar:\n",
    "class TensorDataset(Dataset):\n",
    "    def __init__(self, tensor_directory):\n",
    "        self.tensor_files = [os.path.join(tensor_directory, f) for f in os.listdir(tensor_directory) if f.endswith(\".pt\")]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tensor_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tensor_path = self.tensor_files[idx]\n",
    "        try:\n",
    "            image_tensor, exif_tensor_norm = torch.load(tensor_path, weights_only=True)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading tensor: {tensor_path}, Error: {e}\")\n",
    "            raise e\n",
    "        return image_tensor, exif_tensor_norm\n",
    "\n",
    "\n",
    "#\n",
    "# Definição do modelo, escolha da ResNet e dropout:\n",
    "# -------------------------------------------------\n",
    "#\n",
    "\n",
    "class HighResNetRegressor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(HighResNetRegressor, self).__init__()\n",
    "        self.resnet = selected_resnet\n",
    "\n",
    "        self.resnet.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.dropout = nn.Dropout(p=0.2)  # Add dropout\n",
    "        # self.dropout = nn.Dropout(p=0.35)  # Alterado para ResNet101 e 152, tentando melhorar o overfitting - não funcionou.\n",
    "        \n",
    "        # self.resnet.fc = nn.Linear(self.resnet.fc.in_features, 3)  # Output: [focal_length, f_stop, subject_distance]\n",
    "        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, 2)  # Output: [focal_length, f_stop]         # Dropping subject distance to allow usage of a lens that doesn't include that metadata\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.resnet(x)\n",
    "        x = self.dropout(x)  # Apply dropout\n",
    "        return x\n",
    "\n",
    "\n",
    "print(\"Variáveis inicializadas.\")\n",
    "#print time and date when the script ran. Use Brazilian format:\n",
    "print(\"Executado em\", datetime.datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Salva tensors pré-processados, com dados aumentados, para treinamento posterior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import random\n",
    "\n",
    "# Preprocess the image with augmentation\n",
    "def preprocess_image_with_augmentation(img_path, crop_factor, rotation_angle):\n",
    "    image = Image.open(img_path)\n",
    "    original_size = image.size[0]\n",
    "    crop_size = int(original_size / crop_factor)\n",
    "    rotated_image = image.rotate(random.uniform(rotation_angle/2, rotation_angle), expand=True)\n",
    "    rotated_size = rotated_image.size[0]\n",
    "    left = (rotated_size - crop_size) / 2\n",
    "    top = (rotated_size - crop_size) / 2\n",
    "    right = (rotated_size + crop_size) / 2\n",
    "    bottom = (rotated_size + crop_size) / 2\n",
    "    cropped_image = rotated_image.crop((left, top, right, bottom))\n",
    "    resized_image = cropped_image.resize((final_dimensions, final_dimensions))\n",
    "    transform = transforms.ToTensor()\n",
    "    image_tensor = transform(resized_image)\n",
    "    return image_tensor\n",
    "\n",
    "# Save tensors function with augmentation\n",
    "def save_tensor_with_augmentation(img_path, tensor_directory, crop_factor, rotation_angle, reporter=False):\n",
    "    exif_tensor_raw = extract_exif_data(img_path).float()\n",
    "\n",
    "    # Apply scaling based on crop factor\n",
    "    focal_length_scaled = exif_tensor_raw[0] * crop_factor\n",
    "    f_stop_scaled = exif_tensor_raw[1] * crop_factor\n",
    "    subject_distance_scaled = exif_tensor_raw[2]    # No scaling for subject distance\n",
    "\n",
    "    # Check if the scaled values exceed the limits\n",
    "    if focal_length_scaled > focal_lenght_max or f_stop_scaled > (fstop_max / 3): #don't want f/stop too close to maximum on augmented images\n",
    "        print(f\"Pulando tensor para \\\"{img_path}_{crop_factor}_{rotation_angle}\\\" por estar fora da faixa: {focal_length_scaled:.0f}mm, f/{f_stop_scaled:.1f}\")\n",
    "        return\n",
    "\n",
    "    # Normalize the scaled values\n",
    "    focal_length_norm = normalize_focal_length(focal_length_scaled, focal_lenght_min, focal_lenght_max)\n",
    "    f_stop_norm = normalize_fstop(f_stop_scaled, fstop_min, fstop_max)\n",
    "    subject_distance_norm = normalize_subject_distance(subject_distance_scaled, subject_distance_min, subject_distance_max)\n",
    "\n",
    "    exif_tensor_norm = torch.tensor([focal_length_norm, f_stop_norm, subject_distance_norm]).float()\n",
    "    image_tensor = preprocess_image_with_augmentation(img_path, crop_factor, rotation_angle).half()  # Convert image tensor to FP16\n",
    "\n",
    "    # Save the tensors\n",
    "    tensor_path = os.path.join(tensor_directory, f\"{os.path.basename(img_path).replace('.jpg', '')}_{crop_factor}_{rotation_angle}.pt\")\n",
    "    torch.save((image_tensor, exif_tensor_norm), tensor_path)\n",
    "\n",
    "    if reporter:\n",
    "        global total_tasks, completed_tasks\n",
    "        completed_tasks += 1\n",
    "        if completed_tasks % (total_tasks // 50) == 0:  # Print every 2%\n",
    "            print(f\"Processados {completed_tasks}/{total_tasks} tensors ({completed_tasks / total_tasks * 100:.0f}% completo)\")\n",
    "\n",
    "if not os.path.exists(tensor_directory):\n",
    "    os.makedirs(tensor_directory)\n",
    "\n",
    "# Calculate total number of tasks\n",
    "total_tasks = len(image_files) * len(augmentation_params)\n",
    "completed_tasks = 0\n",
    "\n",
    "# Process each image with all augmentation parameters\n",
    "with ThreadPoolExecutor(max_workers=preprocessing_worker_count) as executor:  \n",
    "    for img_path in image_files:\n",
    "        for crop_factor, rotation_angle in augmentation_params:\n",
    "            executor.submit(save_tensor_with_augmentation, img_path, tensor_directory, crop_factor, rotation_angle, reporter=(completed_tasks == 0))\n",
    "\n",
    "print(\"Tensors salvos com sucesso.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teste de sanidade: verifica se tensors foram salvos corretamente e contém as as informações esperadas.\n",
    "\n",
    "Algumas imagens podem não ter sido convertidas caso os metadados sejam indesejáveis. O código abaixo considera isso e não tenta carregar tensors que não existem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "import os\n",
    "import torch\n",
    "\n",
    "def display_image_with_data(img_path, tensor_directory):\n",
    "    base_name = os.path.basename(img_path).replace(\".jpg\", \"\")\n",
    "    images_to_plot = []\n",
    "\n",
    "    for i, (crop_factor, rotation_angle) in enumerate(augmentation_params):\n",
    "        tensor_path = os.path.join(tensor_directory, f\"{base_name}_{crop_factor}_{rotation_angle}.pt\")\n",
    "\n",
    "        try:\n",
    "            image_tensor, exif_tensor_norm = torch.load(tensor_path, map_location=torch.device('cpu'), weights_only=True)\n",
    "            images_to_plot.append((image_tensor, exif_tensor_norm, crop_factor, rotation_angle))\n",
    "        except FileNotFoundError:\n",
    "            # print(f\"Tensor not found for {base_name}_{crop_factor}_{rotation_angle}.pt\")\n",
    "            continue\n",
    "\n",
    "    num_images = len(images_to_plot)\n",
    "    if num_images == 0:\n",
    "        # print(f\"No tensors found for {base_name}. Skipping...\")\n",
    "        return\n",
    "\n",
    "    num_cols = 6  # Number of columns in the subplot grid\n",
    "    num_rows = (num_images + num_cols - 1) // num_cols  # Calculate number of rows dynamically\n",
    "\n",
    "    fig, axes = plt.subplots(num_rows, num_cols, figsize=(24, 12 * num_rows // num_cols))\n",
    "    fig.suptitle(f\"{base_name}\", fontsize=16)\n",
    "\n",
    "    for i, (image_tensor, exif_tensor_norm, crop_factor, rotation_angle) in enumerate(images_to_plot):\n",
    "        # Convert the image tensor to a PIL image\n",
    "        image = transforms.ToPILImage()(image_tensor)\n",
    "\n",
    "        # Denormalize the EXIF data\n",
    "        focal_length_35mm = denormalize_focal_length(exif_tensor_norm[0].item(), focal_lenght_min, focal_lenght_max)\n",
    "        f_stop_35mm = denormalize_fstop(exif_tensor_norm[1].item(), fstop_min, fstop_max)\n",
    "        subject_distance_35mm = denormalize_subject_distance(exif_tensor_norm[2].item(), subject_distance_min, subject_distance_max)\n",
    "\n",
    "        focal_length_aps_c = focal_length_35mm / 1.5\n",
    "        f_stop_aps_c = f_stop_35mm / 1.5\n",
    "\n",
    "        # Display the image\n",
    "        row, col = divmod(i, num_cols)\n",
    "        ax = axes[row, col]\n",
    "        ax.imshow(image)\n",
    "        ax.axis('off')\n",
    "\n",
    "        # Add labels\n",
    "        ax.set_title(f\"CF: {crop_factor:.2f}, Rot: {rotation_angle}°\\n35mm: {focal_length_35mm:.0f} mm, f/{f_stop_35mm:.1f}\\nAPS-C: {focal_length_aps_c:.0f} mm, f/{f_stop_aps_c:.1f}\\nDist: {subject_distance_35mm:.2f} m\", fontsize=8)\n",
    "\n",
    "    # Remove unused subplots\n",
    "    for i in range(num_images, num_rows * num_cols):\n",
    "        row, col = divmod(i, num_cols)\n",
    "        fig.delaxes(axes[row, col])\n",
    "\n",
    "    # Adjust subplot spacing\n",
    "    plt.subplots_adjust(wspace=0.0, hspace=0.0)\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0.00, 1, 2.25])\n",
    "    plt.show()\n",
    "\n",
    "# Randomly select some images for sanity check\n",
    "num_images_to_check = 6\n",
    "selected_images = random.sample(image_files, num_images_to_check)\n",
    "\n",
    "# Display the selected images and their associated data\n",
    "for img_path in selected_images:\n",
    "    display_image_with_data(img_path, tensor_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verifica a distribuição resultante dos valores de f/stop e FL das imagens do dataset aumentado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# Function to round f-stop to the nearest full f-stop\n",
    "def round_to_full_fstop(f_stop):\n",
    "    full_fstops = [1, 1.4, 2, 2.8, 4, 5.6, 8, 11, 16, 22, 32, 45, 64]\n",
    "    return min(full_fstops, key=lambda x: abs(x - f_stop))\n",
    "\n",
    "# Function to process a chunk of tensors\n",
    "def process_tensors_chunk(chunk, focal_lengths, f_stops, aperture_sizes, is_reporter=False):\n",
    "    for i, filename in enumerate(chunk):\n",
    "        tensor_path = os.path.join(tensor_directory, filename)\n",
    "        try:\n",
    "            _, exif_tensor_norm = torch.load(tensor_path, map_location=torch.device('cpu'), weights_only=True)\n",
    "            focal_length_norm = exif_tensor_norm[0].item()\n",
    "            f_stop_norm = exif_tensor_norm[1].item()\n",
    "\n",
    "            # Denormalize the values\n",
    "            focal_length = denormalize_focal_length(focal_length_norm, focal_lenght_min, focal_lenght_max)\n",
    "            f_stop = denormalize_fstop(f_stop_norm, fstop_min, fstop_max)\n",
    "\n",
    "            focal_lengths.append(focal_length)\n",
    "            f_stops.append(f_stop)\n",
    "            aperture_sizes.append(focal_length / f_stop)  # Calculate physical aperture size\n",
    "\n",
    "            # Print status update if this is the reporter thread\n",
    "            if is_reporter and i % (len(chunk) // 5) == 0:  # Print every 20%\n",
    "                print(f\"Processados {i * plotting_worker_count}/{len(chunk) * plotting_worker_count} tensors ({i / len(chunk) * 100:.0f}% completo)\")\n",
    "        except Exception as e:\n",
    "            print(f\"Falha ao carregar tensor: {tensor_path}, Erro: {e}\")\n",
    "\n",
    "# Load all tensors and extract f-stop and focal length values\n",
    "total_tensors = len([f for f in os.listdir(tensor_directory) if f.endswith(\".pt\")])\n",
    "chunk_size = total_tensors // plotting_worker_count  # Number of tensors per chunk\n",
    "chunks = [os.listdir(tensor_directory)[i:i + chunk_size] for i in range(0, total_tensors, chunk_size)]\n",
    "\n",
    "# Initialize lists to store results\n",
    "f_stops = []\n",
    "focal_lengths = []\n",
    "aperture_sizes = []\n",
    "\n",
    "# Process each chunk in parallel\n",
    "with ThreadPoolExecutor(max_workers=plotting_worker_count) as executor:\n",
    "    futures = []\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        # Only the first thread is the reporter\n",
    "        is_reporter = i == 0\n",
    "        futures.append(executor.submit(process_tensors_chunk, chunk, focal_lengths, f_stops, aperture_sizes, is_reporter))\n",
    "    for future in futures:\n",
    "        future.result()\n",
    "\n",
    "# Plot histograms\n",
    "plt.figure(figsize=(18, 6))\n",
    "\n",
    "# Histogram for f-stop with log scale\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.hist(f_stops, bins=np.logspace(np.log10(min(f_stops)), np.log10(max(f_stops)), 20), edgecolor='black', alpha=0.7)\n",
    "plt.xscale('log')\n",
    "#translate to english: plt.title('Distribuição de imagens por f/stop')\n",
    "plt.title('Image distribution by f/stop')\n",
    "plt.xlabel('f/stop')\n",
    "# plt.ylabel('Número de imagens')\n",
    "plt.ylabel('Number of images')\n",
    "\n",
    "# Customize x-axis ticks for f-stop\n",
    "full_fstops = [1, 1.4, 2, 2.8, 4, 5.6, 8, 11, 16, 22, 32, 45, 64]\n",
    "plt.xticks(full_fstops, [f\"f/{f}\" for f in full_fstops])\n",
    "\n",
    "# Calculate moving average\n",
    "f_stops_sorted = np.sort(f_stops)\n",
    "hist, bin_edges = np.histogram(f_stops_sorted, bins=np.logspace(np.log10(min(f_stops)), np.log10(max(f_stops)), 20))\n",
    "bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
    "\n",
    "# Adjust the window size for the moving average\n",
    "window_size = 7  # Change this value to adjust smoothness\n",
    "moving_avg = np.convolve(hist, np.ones(window_size)/window_size, mode='same')\n",
    "\n",
    "# Polynomial interpolation for smoother curve\n",
    "x = np.log(bin_centers)\n",
    "y = moving_avg\n",
    "coefficients = np.polyfit(x, y, deg=7)  # Fit a cubic polynomial\n",
    "polynomial = np.poly1d(coefficients)\n",
    "x_new = np.linspace(x.min(), x.max(), 100)  # Create more points for a smoother curve\n",
    "y_new = polynomial(x_new)\n",
    "\n",
    "# Plot moving average line with smoother curve\n",
    "# plt.plot(np.exp(x_new), y_new, color='red', label='Média móvel (suavizada)')\n",
    "plt.plot(np.exp(x_new), y_new, color='red', label='Moving average (smoothed)')\n",
    "plt.legend()\n",
    "\n",
    "# Histogram for focal length with log scale\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.hist(focal_lengths, bins=np.logspace(np.log10(min(focal_lengths)), np.log10(max(focal_lengths)), 20), edgecolor='black', alpha=0.7)\n",
    "plt.xscale('log')\n",
    "#rewrite the following in english:\n",
    "# plt.title('Distribuição de imagens por distância focal')\t\n",
    "# plt.xlabel('Distância focal (mm)')\n",
    "# plt.ylabel('Número de imagens')\n",
    "plt.title('Image distribution by focal length')\t\n",
    "plt.xlabel('Focal length (mm)')\n",
    "plt.ylabel('Number of images')\n",
    "\n",
    "# Customize x-axis ticks for focal length\n",
    "plt.xticks([10, 20, 50, 100, 200, 500, 1000, 2000, 5000], [f\"{x} mm\" for x in [10, 20, 50, 100, 200, 500, 1000, 2000, 5000]])\n",
    "\n",
    "# Calculate moving average for focal lengths\n",
    "focal_lengths_sorted = np.sort(focal_lengths)\n",
    "hist_fl, bin_edges_fl = np.histogram(focal_lengths_sorted, bins=np.logspace(np.log10(min(focal_lengths)), np.log10(max(focal_lengths)), 20))\n",
    "bin_centers_fl = (bin_edges_fl[:-1] + bin_edges_fl[1:]) / 2\n",
    "\n",
    "# Adjust the window size for the moving average\n",
    "window_size_fl = 7  # Change this value to adjust smoothness\n",
    "moving_avg_fl = np.convolve(hist_fl, np.ones(window_size_fl)/window_size_fl, mode='same')\n",
    "\n",
    "# Polynomial interpolation for smoother curve\n",
    "x_fl = np.log(bin_centers_fl)\n",
    "y_fl = moving_avg_fl\n",
    "coefficients_fl = np.polyfit(x_fl, y_fl, deg=7)\n",
    "polynomial_fl = np.poly1d(coefficients_fl)\n",
    "x_new_fl = np.linspace(x_fl.min(), x_fl.max(), 100)  # Create more points for a smoother curve\n",
    "y_new_fl = polynomial_fl(x_new_fl)\n",
    "\n",
    "# Plot moving average line with smoother curve\n",
    "# plt.plot(np.exp(x_new_fl), y_new_fl, color='red', label='Média móvel (suavizada)')\n",
    "plt.plot(np.exp(x_new_fl), y_new_fl, color='red', label='Moving average (smoothed)')\n",
    "plt.legend()\n",
    "\n",
    "# Histogram for aperture size with 1mm resolution\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.hist(aperture_sizes, bins=np.arange(min(aperture_sizes), max(aperture_sizes) + 1, 1), edgecolor='black', alpha=0.7)\n",
    "plt.title('Image distribution by pupil size')\n",
    "plt.xlabel('Pupil size (mm)')\n",
    "plt.ylabel('Number of images')\n",
    "\n",
    "# Customize x-axis ticks for aperture size with 10 ticks\n",
    "plt.xticks(np.linspace(min(aperture_sizes), max(aperture_sizes), 12))\n",
    "\n",
    "# Calculate moving average for aperture sizes\n",
    "aperture_sizes_sorted = np.sort(aperture_sizes)\n",
    "hist_ap, bin_edges_ap = np.histogram(aperture_sizes_sorted, bins=np.arange(min(aperture_sizes), max(aperture_sizes) + 1, 1))\n",
    "bin_centers_ap = (bin_edges_ap[:-1] + bin_edges_ap[1:]) / 2\n",
    "\n",
    "# Adjust the window size for the moving average\n",
    "window_size_ap = 7  # Change this value to adjust smoothness\n",
    "moving_avg_ap = np.convolve(hist_ap, np.ones(window_size_ap)/window_size_ap, mode='same')\n",
    "\n",
    "# Polynomial interpolation for smoother curve\n",
    "x_ap = bin_centers_ap\n",
    "y_ap = moving_avg_ap\n",
    "coefficients_ap = np.polyfit(x_ap, y_ap, deg=7) \n",
    "polynomial_ap = np.poly1d(coefficients_ap)\n",
    "x_new_ap = np.linspace(x_ap.min(), x_ap.max(), 100)  # Create more points for a smoother curve\n",
    "y_new_ap = polynomial_ap(x_new_ap)\n",
    "\n",
    "# Plot moving average line with smoother curve\n",
    "# plt.plot(x_new_ap, y_new_ap, color='red', label='Média móvel (suavizada)')\n",
    "plt.plot(x_new_ap, y_new_ap, color='red', label='Moving average (smoothed)')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cria uma ResNet pré-treinada, adapta entrada e saída e inicia treinamento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch.amp import GradScaler, autocast\n",
    "import os\n",
    "import pickle\n",
    "from torchvision.transforms import functional as F\n",
    "import random\n",
    "import datetime\n",
    "\n",
    "# Load dataset\n",
    "dataset = TensorDataset(tensor_directory)\n",
    "\n",
    "# Split dataset into training, validation, and test sets\n",
    "train_size = int(0.7 * len(dataset))\n",
    "val_size = int(0.15 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "# Save the split indices\n",
    "split_indices = {\n",
    "    'train': train_dataset.indices,\n",
    "    'val': val_dataset.indices,\n",
    "    'test': test_dataset.indices\n",
    "}\n",
    "with open(split_indices_path, 'wb') as f:\n",
    "    pickle.dump(split_indices, f)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=dataloader_batch_size, shuffle=True, num_workers=0) #num_workers precisa ser 0.\n",
    "val_loader = DataLoader(val_dataset, batch_size=dataloader_batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "# Initialize the model, loss function, optimizer and scheduler\n",
    "model = HighResNetRegressor().cuda()\n",
    "criterion = nn.L1Loss()  # Use MAE as the loss function\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)  # Add L2 regularization\n",
    "scaler = GradScaler()\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3, factor=0.1)\n",
    "\n",
    "def save_checkpoint(epoch, model, optimizer, scaler, loss, val_loss, checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, f'checkpoint_epoch_{epoch+1}.pth')\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scaler_state_dict': scaler.state_dict(),\n",
    "        'loss': loss,\n",
    "        'val_loss': val_loss\n",
    "    }, checkpoint_path)\n",
    "    # print(f'Checkpoint saved: {checkpoint_path}')\n",
    "\n",
    "# Function to save training metadata\n",
    "def save_training_metadata(metadata, metadata_file):\n",
    "    with open(metadata_file, 'wb') as f:\n",
    "        pickle.dump(metadata, f)\n",
    "\n",
    "# Early stopping\n",
    "best_val_mape = float('inf')  # Initialize with infinity\n",
    "counter = 0 \n",
    "\n",
    "# Freeze the first few layers of the ResNet model\n",
    "for param in model.resnet.layer1.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in model.resnet.layer2.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in model.resnet.layer3.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in model.resnet.layer4.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Print trainable parameters before starting the training\n",
    "print(f\"Iniciando treinamento de ResNet{resnet_version}. Parâmetros treináveis:\")\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"{name}: {param.shape}\")\n",
    "\n",
    "# Training loop with validation and checkpoint saving\n",
    "training_metadata = {'epochs': [], 'train_losses': [], 'val_losses': [], 'train_mape': [], 'val_mape': [], 'train_msle': [], 'val_msle': [], 'train_mse': [], 'val_mse': [], 'start_times': [], 'end_times': [], 'durations': []}\n",
    "unfreeze_epoch = 20 \n",
    "unfreeze_layers = [model.resnet.layer4, model.resnet.layer3]  # Layers to unfreeze sequentially\n",
    "\n",
    "unfreeze_counter = 0\n",
    "\n",
    "# Inside the training loop\n",
    "for epoch in range(max_epochs):\n",
    "    start_time = datetime.datetime.now()\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_mape = 0.0\n",
    "    train_msle = 0.0\n",
    "    train_mse = 0.0  # Initialize MSE for training\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.cuda().half()  # Convert images to FP16\n",
    "        labels = labels.cuda().float()  # Keep labels in FP32\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # In-place hardware-accelerated augmentations\n",
    "        for j in range(images.size(0)):\n",
    "            img = images[j]\n",
    "            img = F.hflip(img) if random.random() > 0.5 else img # Random horizontal flip\n",
    "            img = F.adjust_brightness(img, random.uniform(0.5, 2.0))\n",
    "            img = F.adjust_contrast(img, random.uniform(0.75, 1.25))\n",
    "            images[j] = img\n",
    "\n",
    "        with autocast(device_type='cuda'):\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels[:, :2])  # Only consider the first two columns for focal length and f-stop\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # Calculate MAPE, MSLE, and MSE on GPU\n",
    "        mape = mean_absolute_percentage_error(labels[:, :2], outputs)\n",
    "        msle = mean_squared_logarithmic_error(labels[:, :2], outputs)\n",
    "        mse = nn.functional.mse_loss(outputs, labels[:, :2])  # Calculate MSE\n",
    "        train_mape += mape.item()\n",
    "        train_msle += msle.item()\n",
    "        train_mse += mse.item()\n",
    "\n",
    "        if (i + 1) % 20 == 0:  # Print every 20 steps\n",
    "            print(f'Step [{i + 1}/{len(train_loader)}], Loss: {loss.item():.5f}, MAPE: {mape.item():.2f}%, MSLE: {msle.item():.5f}, MSE: {mse.item():.5f}')\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "    train_mape /= len(train_loader)\n",
    "    train_msle /= len(train_loader)\n",
    "    train_mse /= len(train_loader)  # Average MSE for training\n",
    "    training_metadata['epochs'].append(epoch + 1)\n",
    "    training_metadata['train_losses'].append(train_loss)\n",
    "    training_metadata['train_mape'].append(train_mape)\n",
    "    training_metadata['train_msle'].append(train_msle)\n",
    "    training_metadata['train_mse'].append(train_mse)\n",
    "\n",
    "    # Validation step\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_mape = 0.0\n",
    "    val_msle = 0.0\n",
    "    val_mse = 0.0  # Initialize MSE for validation\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images = images.cuda().half()\n",
    "            labels = labels.cuda().float()\n",
    "            with torch.amp.autocast(device_type='cuda'):\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels[:, :2])  # Only consider the first two columns for focal length and f-stop\n",
    "\n",
    "                # Calculate MAPE, MSLE, and MSE on GPU\n",
    "                mape = mean_absolute_percentage_error(labels[:, :2], outputs)\n",
    "                msle = mean_squared_logarithmic_error(labels[:, :2], outputs)\n",
    "                mse = nn.functional.mse_loss(outputs, labels[:, :2])  # Calculate MSE\n",
    "                val_loss += loss.item()\n",
    "                val_mape += mape.item()\n",
    "                val_msle += msle.item()\n",
    "                val_mse += mse.item()\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    val_mape /= len(val_loader)\n",
    "    val_msle /= len(val_loader)\n",
    "    val_mse /= len(val_loader)  # Average MSE for validation\n",
    "    training_metadata['val_losses'].append(val_loss)\n",
    "    training_metadata['val_mape'].append(val_mape)\n",
    "    training_metadata['val_msle'].append(val_msle)\n",
    "    training_metadata['val_mse'].append(val_mse)\n",
    "\n",
    "    end_time = datetime.datetime.now()\n",
    "    duration = end_time - start_time\n",
    "    training_metadata['start_times'].append(start_time)\n",
    "    training_metadata['end_times'].append(end_time)\n",
    "    training_metadata['durations'].append(duration)\n",
    "\n",
    "    print(f'{start_time.strftime(\"%Y-%m-%d %H:%M:%S\")} - Época {epoch + 1}/{max_epochs} completa. Loss: Treino = {train_loss:.5f} / Validação = {val_loss:.5f} \\tMAPE: Treino = {train_mape:.2f}% / Validação = {val_mape:.2f}% \\tMSLE: Treino = {train_msle:.5f} / Validação = {val_msle:.5f} \\tMSE: Treino = {train_mse:.5f} / Validação = {val_mse:.5f}')\n",
    "\n",
    "\n",
    "    # Update learning rate scheduler\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    # Adaptive unfreezing\n",
    "    if epoch >= unfreeze_epoch and unfreeze_counter < len(unfreeze_layers):\n",
    "        if val_mape < best_val_mape:  # Change to use val_mape\n",
    "            best_val_mape = val_mape  # Update best_val_mape\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "            if counter >= patience:\n",
    "                for param in unfreeze_layers[unfreeze_counter].parameters():\n",
    "                    param.requires_grad = True\n",
    "                unfreeze_counter += 1\n",
    "                unfreeze_epoch = epoch + 10  # Wait for 10 more epochs before unfreezing the next layer\n",
    "                print(f\"Descongelando camada {5 - unfreeze_counter} na época {epoch + 1}\")\n",
    "                counter = 0  # Reset counter after unfreezing\n",
    "\n",
    "    if val_mape < best_val_mape:\n",
    "        best_val_mape = val_mape  # Update best_val_mape\n",
    "        counter = 0\n",
    "    else:\n",
    "        counter += 1\n",
    "        if counter >= patience and unfreeze_counter >= 2:  # Only start early stopping after unfreezing layers 3 and 4\n",
    "            print(\"Parada adiantada.\")\n",
    "            break\n",
    "\n",
    "    # Save checkpoint and metadata\n",
    "    save_checkpoint(epoch, model, optimizer, scaler, train_loss, val_loss, checkpoint_dir)\n",
    "    metadata_file = os.path.join(checkpoint_dir, f'training_metadata.pkl')\n",
    "    save_training_metadata(training_metadata, metadata_file)\n",
    "\n",
    "print(\"Treinamento finalizado. Melhor Validation MAPE: {:.2f}%\".format(best_val_mape))\n",
    "# Unload the model and optimizer from VRAM\n",
    "del model\n",
    "del optimizer\n",
    "del scaler\n",
    "\n",
    "# Clear the cache and run garbage collection\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "# Optionally, you can also reset the CUDA memory to ensure it's cleared\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "print(\"Modelo e otimizador descarregados da VRAM.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cria metadados de resultados de teste para cada época treinada acima:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import pickle\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "# Load the split indices\n",
    "with open(split_indices_path, 'rb') as f:\n",
    "    split_indices = pickle.load(f)\n",
    "\n",
    "dataset = TensorDataset(tensor_directory)\n",
    "test_indices = split_indices['test']\n",
    "test_dataset = Subset(dataset, test_indices)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=0)\n",
    "\n",
    "# Initialize the model\n",
    "model = HighResNetRegressor().cuda()\n",
    "\n",
    "# Initialize the loss function\n",
    "criterion = nn.L1Loss()\n",
    "\n",
    "# Initialize testing metadata\n",
    "testing_metadata = {'epochs': [], 'test_losses': [], 'test_mape': [], 'test_msle': [], 'test_mse': [], 'start_times': [], 'end_times': [], 'durations': []}\n",
    "\n",
    "# Function to evaluate the model on the test set\n",
    "def evaluate_model(epoch):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    test_mape = 0.0\n",
    "    test_msle = 0.0\n",
    "    test_mse = 0.0  # Initialize MSE for testing\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images = images.cuda().half()\n",
    "            labels = labels.cuda().float()\n",
    "            with torch.amp.autocast(device_type='cuda'):\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels[:, :2])  # Only consider the first two columns for focal length and f-stop\n",
    "\n",
    "                # Calculate MAPE, MSLE, and MSE on GPU\n",
    "                mape = mean_absolute_percentage_error(labels[:, :2], outputs)\n",
    "                msle = mean_squared_logarithmic_error(labels[:, :2], outputs)\n",
    "                mse = nn.functional.mse_loss(outputs, labels[:, :2])  # Calculate MSE\n",
    "                test_loss += loss.item()\n",
    "                test_mape += mape.item()\n",
    "                test_msle += msle.item()\n",
    "                test_mse += mse.item()\n",
    "\n",
    "    test_loss /= len(test_loader)\n",
    "    test_mape /= len(test_loader)\n",
    "    test_msle /= len(test_loader)\n",
    "    test_mse /= len(test_loader)  # Average MSE for testing\n",
    "\n",
    "    testing_metadata['epochs'].append(epoch + 1)\n",
    "    testing_metadata['test_losses'].append(test_loss)\n",
    "    testing_metadata['test_mape'].append(test_mape)\n",
    "    testing_metadata['test_msle'].append(test_msle)\n",
    "    testing_metadata['test_mse'].append(test_mse)\n",
    "\n",
    "    return test_loss, test_mape, test_msle, test_mse\n",
    "\n",
    "# Function to save testing metadata\n",
    "def save_testing_metadata(metadata, metadata_file):\n",
    "    with open(metadata_file, 'wb') as f:\n",
    "        pickle.dump(metadata, f)\n",
    "\n",
    "# Evaluate the model on the test set after each epoch during training\n",
    "for epoch in range(max_epochs):\n",
    "    start_time = datetime.datetime.now()\n",
    "\n",
    "    # Load the checkpoint for the current epoch\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, f'checkpoint_epoch_{epoch+1}.pth')\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        checkpoint = torch.load(checkpoint_path,weights_only=True)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    else:\n",
    "        print(f\"Checkpoint not found for epoch {epoch+1}. Skipping evaluation.\")\n",
    "        continue\n",
    "\n",
    "    # Evaluate the model\n",
    "    test_loss, test_mape, test_msle, test_mse = evaluate_model(epoch)\n",
    "\n",
    "    end_time = datetime.datetime.now()\n",
    "    duration = end_time - start_time\n",
    "    testing_metadata['start_times'].append(start_time)\n",
    "    testing_metadata['end_times'].append(end_time)\n",
    "    testing_metadata['durations'].append(duration)\n",
    "\n",
    "    print(f'{start_time.strftime(\"%Y-%m-%d %H:%M:%S\")} - Época {epoch + 1}/{max_epochs} completa. Test Loss: {test_loss:.5f} \\tTest MAPE: {test_mape:.2f}% \\tTest MSLE: {test_msle:.5f} \\tTest MSE: {test_mse:.5f}')\n",
    "\n",
    "    # Save testing metadata\n",
    "    metadata_file = os.path.join(checkpoint_dir, f'testing_metadata.pkl')\n",
    "    save_testing_metadata(testing_metadata, metadata_file)\n",
    "\n",
    "# Unload the model and optimizer from VRAM\n",
    "del model\n",
    "\n",
    "# Clear the cache and run garbage collection\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "# Optionally, you can also reset the CUDA memory to ensure it's cleared\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "print(\"Avaliação no conjunto de teste concluída.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gera gráficos de loss, MAPE e MSLE por época:\n",
    "\n",
    "Em seguida, testa o treco com o melhor checkpoint disponível conforme cada métrica:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# Load the training metadata\n",
    "metadata_file = os.path.join(checkpoint_dir, 'training_metadata.pkl')\n",
    "with open(metadata_file, 'rb') as f:\n",
    "    training_metadata = pickle.load(f)\n",
    "\n",
    "# Load the testing metadata\n",
    "testing_metadata_file = os.path.join(checkpoint_dir, 'testing_metadata.pkl')\n",
    "with open(testing_metadata_file, 'rb') as f:\n",
    "    testing_metadata = pickle.load(f)\n",
    "\n",
    "# Extract the data\n",
    "\n",
    "epochs = training_metadata['epochs']\n",
    "train_losses = training_metadata['train_losses']\n",
    "val_losses = training_metadata['val_losses']\n",
    "test_losses = testing_metadata['test_losses']\n",
    "train_mape = training_metadata['train_mape']\n",
    "val_mape = training_metadata['val_mape']\n",
    "test_mape = testing_metadata['test_mape']\n",
    "train_msle = training_metadata['train_msle']\n",
    "val_msle = training_metadata['val_msle']\n",
    "test_msle = testing_metadata['test_msle']\n",
    "train_mse = training_metadata['train_mse']\n",
    "val_mse = training_metadata['val_mse']\n",
    "test_mse = testing_metadata['test_mse']\n",
    "\n",
    "\n",
    "# Number of initial epochs to skip\n",
    "skip_epochs = 0  # Adjust this number as needed\n",
    "\n",
    "# Slice the data to skip the initial epochs, if needed:\n",
    "epochs = epochs[skip_epochs:len(test_losses)]   #workaround to fix the bug of the last epoch not being saved in the testing metadata\n",
    "train_losses = train_losses[skip_epochs:len(epochs)]\n",
    "val_losses = val_losses[skip_epochs:len(epochs)]\n",
    "test_losses = test_losses[skip_epochs:len(epochs)]\n",
    "train_mape = train_mape[skip_epochs:len(epochs)]\n",
    "val_mape = val_mape[skip_epochs:len(epochs)]\n",
    "test_mape = test_mape[skip_epochs:len(epochs)]\n",
    "train_msle = train_msle[skip_epochs:len(epochs)]\n",
    "val_msle = val_msle[skip_epochs:len(epochs)]\n",
    "test_msle = test_msle[skip_epochs:len(epochs)]\n",
    "train_mse = train_mse[skip_epochs:len(epochs)]\n",
    "val_mse = val_mse[skip_epochs:len(epochs)]\n",
    "test_mse = test_mse[skip_epochs:len(epochs)]\n",
    "\n",
    "# Find the best epoch based on test performance\n",
    "best_epoch_idx_loss = test_losses.index(min(test_losses))\n",
    "best_epoch_idx_mape = test_mape.index(min(test_mape))\n",
    "best_epoch_idx_msle = test_msle.index(min(test_msle))\n",
    "best_epoch_idx_mse = test_mse.index(min(test_mse))\n",
    "\n",
    "best_epoch_loss = epochs[best_epoch_idx_loss]\n",
    "best_train_loss_loss = train_losses[best_epoch_idx_loss]\n",
    "best_val_loss_loss = val_losses[best_epoch_idx_loss]\n",
    "best_test_loss_loss = test_losses[best_epoch_idx_loss]\n",
    "best_train_mape_loss = train_mape[best_epoch_idx_loss]\n",
    "best_val_mape_loss = val_mape[best_epoch_idx_loss]\n",
    "best_test_mape_loss = test_mape[best_epoch_idx_loss]\n",
    "best_train_msle_loss = train_msle[best_epoch_idx_loss]\n",
    "best_val_msle_loss = val_msle[best_epoch_idx_loss]\n",
    "best_test_msle_loss = test_msle[best_epoch_idx_loss]\n",
    "best_train_mse_loss = train_mse[best_epoch_idx_loss]\n",
    "best_val_mse_loss = val_mse[best_epoch_idx_loss]\n",
    "best_test_mse_loss = test_mse[best_epoch_idx_loss]\n",
    "\n",
    "best_epoch_mape = epochs[best_epoch_idx_mape]\n",
    "best_train_loss_mape = train_losses[best_epoch_idx_mape]\n",
    "best_val_loss_mape = val_losses[best_epoch_idx_mape]\n",
    "best_test_loss_mape = test_losses[best_epoch_idx_mape]\n",
    "best_train_mape_mape = train_mape[best_epoch_idx_mape]\n",
    "best_val_mape_mape = val_mape[best_epoch_idx_mape]\n",
    "best_test_mape_mape = test_mape[best_epoch_idx_mape]\n",
    "best_train_msle_mape = train_msle[best_epoch_idx_mape]\n",
    "best_val_msle_mape = val_msle[best_epoch_idx_mape]\n",
    "best_test_msle_mape = test_msle[best_epoch_idx_mape]\n",
    "best_train_mse_mape = train_mse[best_epoch_idx_mape]\n",
    "best_val_mse_mape = val_mse[best_epoch_idx_mape]\n",
    "best_test_mse_mape = test_mse[best_epoch_idx_mape]\n",
    "\n",
    "best_epoch_msle = epochs[best_epoch_idx_msle]\n",
    "best_train_loss_msle = train_losses[best_epoch_idx_msle]\n",
    "best_val_loss_msle = val_losses[best_epoch_idx_msle]\n",
    "best_test_loss_msle = test_losses[best_epoch_idx_msle]\n",
    "best_train_mape_msle = train_mape[best_epoch_idx_msle]\n",
    "best_val_mape_msle = val_mape[best_epoch_idx_msle]\n",
    "best_test_mape_msle = test_mape[best_epoch_idx_msle]\n",
    "best_train_msle_msle = train_msle[best_epoch_idx_msle]\n",
    "best_val_msle_msle = val_msle[best_epoch_idx_msle]\n",
    "best_test_msle_msle = test_msle[best_epoch_idx_msle]\n",
    "best_train_mse_msle = train_mse[best_epoch_idx_msle]\n",
    "best_val_mse_msle = val_mse[best_epoch_idx_msle]\n",
    "best_test_mse_msle = test_mse[best_epoch_idx_msle]\n",
    "\n",
    "best_epoch_mse = epochs[best_epoch_idx_mse]\n",
    "best_train_loss_mse = train_losses[best_epoch_idx_mse]\n",
    "best_val_loss_mse = val_losses[best_epoch_idx_mse]\n",
    "best_test_loss_mse = test_losses[best_epoch_idx_mse]\n",
    "best_train_mape_mse = train_mape[best_epoch_idx_mse]\n",
    "best_val_mape_mse = val_mape[best_epoch_idx_mse]\n",
    "best_test_mape_mse = test_mape[best_epoch_idx_mse]\n",
    "best_train_msle_mse = train_msle[best_epoch_idx_mse]\n",
    "best_val_msle_mse = val_msle[best_epoch_idx_mse]\n",
    "best_test_msle_mse = test_msle[best_epoch_idx_mse]\n",
    "best_train_mse_mse = train_mse[best_epoch_idx_mse]\n",
    "best_val_mse_mse = val_mse[best_epoch_idx_mse]\n",
    "best_test_mse_mse = test_mse[best_epoch_idx_mse]\n",
    "\n",
    "# Print the best epoch details for each metric\n",
    "print(f\"Melhor época por MAPE (Teste): {best_epoch_mape}\")\n",
    "print(f\"Loss:\\t Treino = {best_train_loss_mape:.5f} \\t Validação = {best_val_loss_mape:.5f} \\t Teste = {best_test_loss_mape:.5f}\")\n",
    "print(f\"MAPE:\\t Treino = {best_train_mape_mape:.4f}% \\t Validação = {best_val_mape_mape:.4f}% \\t Teste = {best_test_mape_mape:.4f}%\")\n",
    "print(f\"MSLE:\\t Treino = {best_train_msle_mape:.5f} \\t Validação = {best_val_msle_mape:.5f} \\t Teste = {best_test_msle_mape:.5f}\")\n",
    "print(f\"MSE:\\t Treino = {best_train_mse_mape:.5f} \\t Validação = {best_val_mse_mape:.5f} \\t Teste = {best_test_mse_mape:.5f}\")\n",
    "\n",
    "print(f\"\\nMelhor época por Loss (Teste): {best_epoch_loss}\")\n",
    "print(f\"Loss:\\t Treino = {best_train_loss_loss:.5f} \\t Validação = {best_val_loss_loss:.5f} \\t Teste = {best_test_loss_loss:.5f}\")\n",
    "print(f\"MAPE:\\t Treino = {best_train_mape_loss:.4f}% \\t Validação = {best_val_mape_loss:.4f}% \\t Teste = {best_test_mape_loss:.4f}%\")\n",
    "print(f\"MSLE:\\t Treino = {best_train_msle_loss:.5f} \\t Validação = {best_val_msle_loss:.5f} \\t Teste = {best_test_msle_loss:.5f}\")\n",
    "print(f\"MSE:\\t Treino = {best_train_mse_loss:.5f} \\t Validação = {best_val_mse_loss:.5f} \\t Teste = {best_test_mse_loss:.5f}\")\n",
    "\n",
    "print(f\"\\nMelhor época por MSLE (Teste): {best_epoch_msle}\")\n",
    "print(f\"Loss:\\t Treino = {best_train_loss_msle:.5f} \\t Validação = {best_val_loss_msle:.5f} \\t Teste = {best_test_loss_msle:.5f}\")\n",
    "print(f\"MAPE:\\t Treino = {best_train_mape_msle:.4f}% \\t Validação = {best_val_mape_msle:.4f}% \\t Teste = {best_test_mape_msle:.4f}%\")\n",
    "print(f\"MSLE:\\t Treino = {best_train_msle_msle:.5f} \\t Validação = {best_val_msle_msle:.5f} \\t Teste = {best_test_msle_msle:.5f}\")\n",
    "print(f\"MSE:\\t Treino = {best_train_mse_msle:.5f} \\t Validação = {best_val_mse_msle:.5f} \\t Teste = {best_test_mse_msle:.5f}\")\n",
    "\n",
    "print(f\"\\nMelhor época por MSE (Teste): {best_epoch_mse}\")\n",
    "print(f\"Loss:\\t Treino = {best_train_loss_mse:.5f} \\t Validação = {best_val_loss_mse:.5f} \\t Teste = {best_test_loss_mse:.5f}\")\n",
    "print(f\"MAPE:\\t Treino = {best_train_mape_mse:.4f}% \\t Validação = {best_val_mape_mse:.4f}% \\t Teste = {best_test_mape_mse:.4f}%\")\n",
    "print(f\"MSLE:\\t Treino = {best_train_msle_mse:.5f} \\t Validação = {best_val_msle_mse:.5f} \\t Teste = {best_test_msle_mse:.5f}\")\n",
    "print(f\"MSE:\\t Treino = {best_train_mse_mse:.5f} \\t Validação = {best_val_mse_mse:.5f} \\t Teste = {best_test_mse_mse:.5f}\")\n",
    "\n",
    "# Plot training, validation, and test losses, along with MAPE, MSLE, and MSE, highlighting the best epoch with a marker\n",
    "plt.figure(figsize=(18, 12))\n",
    "\n",
    "# Loss Plot\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(epochs, train_losses, label='Training Loss', marker='o')\n",
    "plt.plot(epochs, val_losses, label='Validation Loss', marker='x')\n",
    "plt.plot(epochs, test_losses, label='Test Loss', marker='^')\n",
    "plt.plot(best_epoch_loss, best_test_loss_loss, 'ro', label='Best Epoch by Loss', markersize=12)\n",
    "plt.xlabel('Epochs', fontsize=16)\n",
    "plt.ylabel('MAE Loss', fontsize=16)\n",
    "plt.title('MAE Loss per Epoch', fontsize=16)\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True)\n",
    "\n",
    "# MAPE Plot\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(epochs, train_mape, label='Training MAPE', marker='o')\n",
    "plt.plot(epochs, val_mape, label='Validation MAPE', marker='x')\n",
    "plt.plot(epochs, test_mape, label='Test MAPE', marker='^')\n",
    "plt.plot(best_epoch_mape, best_test_mape_mape, 'ro', label='Best Epoch by MAPE', markersize=12)\n",
    "plt.xlabel('Epochs', fontsize=16)\n",
    "plt.ylabel('MAPE (%)', fontsize=16)\n",
    "plt.title('MAPE per Epoch', fontsize=18)\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True)\n",
    "\n",
    "# MSLE Plot\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(epochs, train_msle, label='Training MSLE', marker='o')\n",
    "plt.plot(epochs, val_msle, label='Validation MSLE', marker='x')\n",
    "plt.plot(epochs, test_msle, label='Test MSLE', marker='^')\n",
    "plt.plot(best_epoch_msle, best_test_msle_msle, 'ro', label='Best Epoch by MSLE', markersize=12)\n",
    "plt.xlabel('Epochs', fontsize=16)\n",
    "plt.ylabel('MSLE', fontsize=16)\n",
    "plt.title('MSLE per Epoch', fontsize=18)\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True)\n",
    "\n",
    "# MSE Plot\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(epochs, train_mse, label='Training MSE', marker='o')\n",
    "plt.plot(epochs, val_mse, label='Validation MSE', marker='x')\n",
    "plt.plot(epochs, test_mse, label='Test MSE', marker='^')\n",
    "plt.plot(best_epoch_mse, best_test_mse_mse, 'ro', label='Best Epoch by MSE', markersize=12)\n",
    "plt.xlabel('Epochs', fontsize=16)\n",
    "plt.ylabel('MSE', fontsize=16)\n",
    "plt.title('MSE per Epoch', fontsize=18)\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "\n",
    "# Load the training metadata\n",
    "metadata_file = os.path.join(checkpoint_dir, 'training_metadata.pkl')\n",
    "with open(metadata_file, 'rb') as f:\n",
    "    training_metadata = pickle.load(f)\n",
    "\n",
    "# Load the testing metadata\n",
    "testing_metadata_file = os.path.join(checkpoint_dir, 'testing_metadata.pkl')\n",
    "with open(testing_metadata_file, 'rb') as f:\n",
    "    testing_metadata = pickle.load(f)\n",
    "\n",
    "# Function to evaluate the model and return individual errors\n",
    "def evaluate_model(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    all_real_metadata = []\n",
    "    all_inferred_metadata = []\n",
    "    focal_length_errors = []\n",
    "    fstop_errors = []\n",
    "    all_images = []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images = images.cuda().half()\n",
    "            labels = labels.cuda().float()\n",
    "            with torch.amp.autocast(device_type='cuda'):\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels[:, :2])  # Only consider the first two columns for focal length and f-stop\n",
    "                test_loss += loss.item()\n",
    "                all_real_metadata.extend(labels.cpu().numpy())\n",
    "                all_inferred_metadata.extend(outputs.cpu().numpy())\n",
    "                focal_length_errors.extend(calculate_percentage_error(labels[:, 0].cpu().numpy(), outputs[:, 0].cpu().numpy()))\n",
    "                fstop_errors.extend(calculate_percentage_error(labels[:, 1].cpu().numpy(), outputs[:, 1].cpu().numpy()))\n",
    "                all_images.extend(images.cpu())\n",
    "    test_loss /= len(test_loader)\n",
    "    print(f'Test Loss: {test_loss:.5f}')\n",
    "\n",
    "    all_real_metadata = np.array(all_real_metadata)\n",
    "    all_inferred_metadata = np.array(all_inferred_metadata)\n",
    "    focal_length_errors = np.array(focal_length_errors)\n",
    "    fstop_errors = np.array(fstop_errors)\n",
    "\n",
    "    avg_focal_length_error = np.mean(focal_length_errors)\n",
    "    avg_fstop_error = np.mean(fstop_errors)\n",
    "\n",
    "    print(f'Erro percentual médio - FL: {avg_focal_length_error:.2f}%')\n",
    "    print(f'Erro percentual médio - f/stop: {avg_fstop_error:.2f}%')\n",
    "\n",
    "    return all_images, all_real_metadata, all_inferred_metadata, focal_length_errors, fstop_errors\n",
    "\n",
    "# Function to visualize the results in a grid format\n",
    "def visualize_results(images, real_metadata, inferred_metadata, num_samples=32, title=\"\"):\n",
    "    num_images = min(num_samples, len(images))\n",
    "    num_cols = 6  # Number of columns in the subplot grid\n",
    "    num_rows = (num_images + num_cols - 1) // num_cols  # Calculate number of rows dynamically\n",
    "\n",
    "    fig, axes = plt.subplots(num_rows, num_cols, figsize=(24, 12 * num_rows // num_cols))\n",
    "    fig.suptitle(title, fontsize=16, y=1.05)  # Adjust the y parameter to move the title up\n",
    "\n",
    "    for i in range(num_images):\n",
    "        image = images[i].numpy().transpose(1, 2, 0)  # Convert to HWC format\n",
    "        image = image.astype('float32')  # Ensure the image is of type float32\n",
    "\n",
    "        real_focal_length = denormalize_focal_length(real_metadata[i][0], focal_lenght_min, focal_lenght_max)\n",
    "        real_fstop = denormalize_fstop(real_metadata[i][1], fstop_min, fstop_max)\n",
    "        inferred_focal_length = denormalize_focal_length(inferred_metadata[i][0], focal_lenght_min, focal_lenght_max)\n",
    "        inferred_fstop = denormalize_fstop(inferred_metadata[i][1], fstop_min, fstop_max)\n",
    "\n",
    "        # Ensure values are within expected ranges\n",
    "        inferred_focal_length = max(focal_lenght_min, min(focal_lenght_max, inferred_focal_length))\n",
    "        inferred_fstop = max(fstop_min, min(fstop_max, inferred_fstop))\n",
    "\n",
    "        # Convert the image tensor to a PIL image\n",
    "        pil_image = transforms.ToPILImage()(image)\n",
    "\n",
    "        # Display the image\n",
    "        row, col = divmod(i, num_cols)\n",
    "        if num_rows == 1:\n",
    "            ax = axes[col]\n",
    "        else:\n",
    "            ax = axes[row, col]\n",
    "        ax.imshow(pil_image)\n",
    "        ax.axis('off')\n",
    "\n",
    "        # Add labels\n",
    "        ax.set_title(f\"Real: FL={real_focal_length:.0f}mm, f/{real_fstop:.1f}\\nInferred: FL={inferred_focal_length:.0f}mm, f/{inferred_fstop:.1f}\", fontsize=13)\n",
    "\n",
    "    # Remove unused subplots\n",
    "    for i in range(num_images, num_rows * num_cols):\n",
    "        row, col = divmod(i, num_cols)\n",
    "        if num_rows == 1:\n",
    "            fig.delaxes(axes[col])\n",
    "        else:\n",
    "            fig.delaxes(axes[row, col])\n",
    "\n",
    "    # Adjust subplot spacing\n",
    "    plt.subplots_adjust(wspace=0.05, hspace=0.05)\n",
    "    plt.tight_layout(rect=[0, 0.00, 1, 2.25]) \n",
    "    plt.show()\n",
    "\n",
    "# Function to load and evaluate the model\n",
    "def load_and_evaluate_model(checkpoint_path, test_loader):\n",
    "    # Load the checkpoint\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=torch.device('cpu'), weights_only=True)\n",
    "    model = HighResNetRegressor().cuda()\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    criterion = nn.L1Loss()  # Define the criterion here\n",
    "\n",
    "    # Evaluate the model\n",
    "    all_images, all_real_metadata, all_inferred_metadata, focal_length_errors, fstop_errors = evaluate_model(model, test_loader, criterion)\n",
    "\n",
    "    # Unload the model and optimizer from VRAM\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "    return all_images, all_real_metadata, all_inferred_metadata, focal_length_errors, fstop_errors\n",
    "\n",
    "# Extract the data\n",
    "epochs = training_metadata['epochs']\n",
    "train_losses = training_metadata['train_losses']\n",
    "val_losses = training_metadata['val_losses']\n",
    "test_losses = testing_metadata['test_losses']\n",
    "train_mape = training_metadata['train_mape']\n",
    "val_mape = training_metadata['val_mape']\n",
    "test_mape = testing_metadata['test_mape']\n",
    "train_msle = training_metadata['train_msle']\n",
    "val_msle = training_metadata['val_msle']\n",
    "test_msle = testing_metadata['test_msle']\n",
    "\n",
    "# Number of initial epochs to skip\n",
    "skip_epochs = 0  # Adjust this number as needed\n",
    "\n",
    "# Slice the data to skip the initial epochs\n",
    "epochs = epochs[skip_epochs:len(epochs)]\n",
    "train_losses = train_losses[skip_epochs:len(epochs)]\n",
    "val_losses = val_losses[skip_epochs:len(epochs)]\n",
    "test_losses = test_losses[skip_epochs:len(epochs)]\n",
    "train_mape = train_mape[skip_epochs:len(epochs)]\n",
    "val_mape = val_mape[skip_epochs:len(epochs)]\n",
    "test_mape = test_mape[skip_epochs:len(epochs)]\n",
    "train_msle = train_msle[skip_epochs:len(epochs)]\n",
    "val_msle = val_msle[skip_epochs:len(epochs)]\n",
    "test_msle = test_msle[skip_epochs:len(epochs)]\n",
    "\n",
    "# Identify the best epoch based on the lowest test MAPE, Loss, and MSLE\n",
    "best_epoch_idx_mape = test_mape.index(min(test_mape))\n",
    "best_epoch_mape = epochs[best_epoch_idx_mape]\n",
    "\n",
    "best_epoch_idx_loss = test_losses.index(min(test_losses))\n",
    "best_epoch_loss = epochs[best_epoch_idx_loss]\n",
    "\n",
    "best_epoch_idx_msle = test_msle.index(min(test_msle))\n",
    "best_epoch_msle = epochs[best_epoch_idx_msle]\n",
    "\n",
    "# Load the split indices\n",
    "with open(split_indices_path, 'rb') as f:\n",
    "    split_indices = pickle.load(f)\n",
    "\n",
    "dataset = TensorDataset(tensor_directory)\n",
    "test_indices = split_indices['test']\n",
    "test_dataset = Subset(dataset, test_indices)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=0)\n",
    "\n",
    "# Evaluate the model at the best epoch by MAPE\n",
    "checkpoint_path_mape = os.path.join(checkpoint_dir, f'checkpoint_epoch_{best_epoch_mape}.pth')\n",
    "print(f\"\\nResNet{resnet_version}: \\nAvaliando melhor época por MAPE (Teste): {best_epoch_mape}\")\n",
    "all_images_mape, all_real_metadata_mape, all_inferred_metadata_mape, focal_length_errors_mape, fstop_errors_mape = load_and_evaluate_model(checkpoint_path_mape, test_loader)\n",
    "\n",
    "# Visualize the best and worst results for the best epoch by MAPE\n",
    "combined_errors_mape = focal_length_errors_mape + fstop_errors_mape\n",
    "sorted_indices_mape = np.argsort(combined_errors_mape)\n",
    "best_indices_mape = sorted_indices_mape[:5]\n",
    "worst_indices_mape = sorted_indices_mape[-5:]\n",
    "print(\"\\nMelhores resultados por MAPE (Teste):\")\n",
    "visualize_results([all_images_mape[i] for i in best_indices_mape], all_real_metadata_mape[best_indices_mape], all_inferred_metadata_mape[best_indices_mape], num_samples=5)\n",
    "print(\"\\nPiores resultados por MAPE (Teste):\")\n",
    "visualize_results([all_images_mape[i] for i in worst_indices_mape], all_real_metadata_mape[worst_indices_mape], all_inferred_metadata_mape[worst_indices_mape], num_samples=5)\n",
    "print(\"\\nResultados sortidos por MAPE (Teste):\")    # Visualize assorted results only for the best epoch by MAPE\n",
    "visualize_results(all_images_mape, all_real_metadata_mape, all_inferred_metadata_mape, num_samples=32)\n",
    "\n",
    "# Evaluate the model at the best epoch by loss\n",
    "checkpoint_path_loss = os.path.join(checkpoint_dir, f'checkpoint_epoch_{best_epoch_loss}.pth')\n",
    "print(f\"\\nAvaliando melhor época por Loss (Teste): {best_epoch_loss}\")\n",
    "all_images_loss, all_real_metadata_loss, all_inferred_metadata_loss, focal_length_errors_loss, fstop_errors_loss = load_and_evaluate_model(checkpoint_path_loss, test_loader)\n",
    "\n",
    "# Visualize the best and worst results for the best epoch by Loss\n",
    "combined_errors_loss = focal_length_errors_loss + fstop_errors_loss\n",
    "sorted_indices_loss = np.argsort(combined_errors_loss)\n",
    "best_indices_loss = sorted_indices_loss[:5]\n",
    "worst_indices_loss = sorted_indices_loss[-5:]\n",
    "print(\"\\nMelhores resultados por Loss (Teste):\")\n",
    "visualize_results([all_images_loss[i] for i in best_indices_loss], all_real_metadata_loss[best_indices_loss], all_inferred_metadata_loss[best_indices_loss], num_samples=5)\n",
    "print(\"\\nPiores resultados por Loss (Teste):\")\n",
    "visualize_results([all_images_loss[i] for i in worst_indices_loss], all_real_metadata_loss[worst_indices_loss], all_inferred_metadata_loss[worst_indices_loss], num_samples=5)\n",
    "\n",
    "# Evaluate the model at the best epoch by MSLE\n",
    "checkpoint_path_msle = os.path.join(checkpoint_dir, f'checkpoint_epoch_{best_epoch_msle}.pth')\n",
    "print(f\"\\nAvaliando melhor época por MSLE (Teste): {best_epoch_msle}\")\n",
    "all_images_msle, all_real_metadata_msle, all_inferred_metadata_msle, focal_length_errors_msle, fstop_errors_msle = load_and_evaluate_model(checkpoint_path_msle, test_loader)\n",
    "\n",
    "# Visualize the best and worst results for the best epoch by MSLE\n",
    "combined_errors_msle = focal_length_errors_msle + fstop_errors_msle\n",
    "sorted_indices_msle = np.argsort(combined_errors_msle)\n",
    "best_indices_msle = sorted_indices_msle[:5]\n",
    "worst_indices_msle = sorted_indices_msle[-5:]\n",
    "print(\"\\nMelhores resultados por MSLE (Teste):\")\n",
    "visualize_results([all_images_msle[i] for i in best_indices_msle], all_real_metadata_msle[best_indices_msle], all_inferred_metadata_msle[best_indices_msle], num_samples=5)\n",
    "print(\"\\nPiores resultados por MSLE (Teste):\")\n",
    "visualize_results([all_images_msle[i] for i in worst_indices_msle], all_real_metadata_msle[worst_indices_msle], all_inferred_metadata_msle[worst_indices_msle], num_samples=5)\n",
    "\n",
    "# Evaluate the model at the last epoch found in the folder:\n",
    "last_epoch = max([int(filename.split('_')[-1].split('.')[0]) for filename in os.listdir(checkpoint_dir) if filename.startswith('checkpoint_epoch_')])\n",
    "checkpoint_path_last = os.path.join(checkpoint_dir, f'checkpoint_epoch_{last_epoch}.pth')\n",
    "print(f\"\\nAvaliando última época encontrada: {last_epoch}\")\n",
    "all_images_last, all_real_metadata_last, all_inferred_metadata_last, focal_length_errors_last, fstop_errors_last = load_and_evaluate_model(checkpoint_path_last, test_loader)\n",
    "\n",
    "# Visualize the best and worst results for the best epoch by last\n",
    "combined_errors_last = focal_length_errors_last + fstop_errors_last\n",
    "sorted_indices_last = np.argsort(combined_errors_last)\n",
    "best_indices_last = sorted_indices_last[:5]\n",
    "worst_indices_last = sorted_indices_last[-5:]\n",
    "print(\"\\nMelhores resultados por last (Teste):\")\n",
    "visualize_results([all_images_last[i] for i in best_indices_last], all_real_metadata_last[best_indices_last], all_inferred_metadata_last[best_indices_last], num_samples=5)\n",
    "print(\"\\nPiores resultados por last (Teste):\")\n",
    "visualize_results([all_images_last[i] for i in worst_indices_last], all_real_metadata_last[worst_indices_last], all_inferred_metadata_last[worst_indices_last], num_samples=5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Error trends, residuals, etc.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Plot error trends with real 35mm-equivalent values\n",
    "plt.figure(figsize=(18, 6))\n",
    "\n",
    "# Plot focal length error vs true focal length\n",
    "plt.subplot(1, 2, 1)\n",
    "real_focal_lengths_mape = [denormalize_focal_length(val, focal_lenght_min, focal_lenght_max) for val in all_real_metadata_mape[:, 0]]\n",
    "plt.scatter(real_focal_lengths_mape, focal_length_errors_mape, alpha=0.5)\n",
    "plt.xscale('log')\n",
    "plt.xlabel('True Focal Length (35mm-equiv)')\n",
    "plt.ylabel('Percentage Error (%)')\n",
    "plt.title('Focal Length Error Trend')\n",
    "plt.xticks([16, 20, 28, 35, 50, 70, 85, 105, 135, 200, 250, 300, 400, 500, 600], [f\"{x}\" for x in [16, 20, 28, 35, 50, 70, 85, 105, 135, 200, 250, 300, 400, 500, 600]])\n",
    "\n",
    "# Plot f-stop error vs true f-stop\n",
    "plt.subplot(1, 2, 2)\n",
    "real_fstops_mape = [denormalize_fstop(val, fstop_min, fstop_max) for val in all_real_metadata_mape[:, 1]]\n",
    "plt.scatter(real_fstops_mape, fstop_errors_mape, alpha=0.5)\n",
    "plt.xscale('log')\n",
    "plt.xlabel('True f-stop')\n",
    "plt.ylabel('Percentage Error (%)')\n",
    "plt.title('f-stop Error Trend')\n",
    "plt.xticks([2, 2.8, 4, 5.6, 8, 11, 16], [f\"f/{f}\" for f in [2, 2.8, 4, 5.6, 8, 11, 16]])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate residuals\n",
    "real_focal_lengths_mape = [denormalize_focal_length(val, focal_lenght_min, focal_lenght_max) for val in all_real_metadata_mape[:, 0]]\n",
    "predicted_focal_lengths_mape = [denormalize_focal_length(val, focal_lenght_min, focal_lenght_max) for val in all_inferred_metadata_mape[:, 0]]\n",
    "residuals_fl = np.array(real_focal_lengths_mape) - np.array(predicted_focal_lengths_mape)\n",
    "\n",
    "real_fstops_mape = [denormalize_fstop(val, fstop_min, fstop_max) for val in all_real_metadata_mape[:, 1]]\n",
    "predicted_fstops_mape = [denormalize_fstop(val, fstop_min, fstop_max) for val in all_inferred_metadata_mape[:, 1]]\n",
    "residuals_fstop = np.array(real_fstops_mape) - np.array(predicted_fstops_mape)\n",
    "\n",
    "# Plot residuals vs actual values\n",
    "plt.figure(figsize=(18, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(real_focal_lengths_mape, residuals_fl, alpha=0.5)\n",
    "plt.xlabel('True Focal Length (35mm-equiv)')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residuals vs True Focal Length')\n",
    "plt.xscale('log')\n",
    "plt.xticks([16, 20, 28, 35, 50, 70, 85, 105, 135, 200, 250, 300, 400, 500, 600], [f\"{x}\" for x in [16, 20, 28, 35, 50, 70, 85, 105, 135, 200, 250, 300, 400, 500, 600]])\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(real_fstops_mape, residuals_fstop, alpha=0.5)\n",
    "plt.xlabel('True f-stop')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residuals vs True f-stop')\n",
    "plt.xscale('log')\n",
    "plt.xticks([2, 2.8, 4, 5.6, 8, 11, 16], [f\"f/{f}\" for f in [2, 2.8, 4, 5.6, 8, 11, 16]])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Bias Analysis: Mean error for different ranges\n",
    "def mean_error_in_range(real_values, predicted_values, min_val, max_val):\n",
    "    real_values = np.array(real_values)\n",
    "    predicted_values = np.array(predicted_values)\n",
    "    mask = (real_values >= min_val) & (real_values < max_val)\n",
    "    if mask.any():  # Check if there are any values in the range\n",
    "        errors = (predicted_values[mask] - real_values[mask]) / real_values[mask] * 100\n",
    "        return np.mean(errors)\n",
    "    else:\n",
    "        return np.nan  # Return NaN if there are no values in the range\n",
    "\n",
    "# Define ranges for focal lengths and f-stops\n",
    "fl_ranges = [(16, 24), (24, 35), (35, 50), (50, 70), (70, 85), (85, 105), (105, 135), (135, 200), (200, 250), (250, 300), (300, 400), (400, 500), (500, 600)]\n",
    "fstop_ranges = [(2, 2.8), (2.8, 4), (4, 5.6), (5.6, 8), (8, 11), (11, 16)]\n",
    "\n",
    "# Calculate mean errors for focal lengths\n",
    "mean_errors_fl = [mean_error_in_range(real_focal_lengths_mape, predicted_focal_lengths_mape, min_val, max_val) for min_val, max_val in fl_ranges]\n",
    "\n",
    "# Plot mean errors for focal lengths\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(range(len(fl_ranges)), mean_errors_fl, tick_label=[f\"{min_val}-{max_val}\" for min_val, max_val in fl_ranges])\n",
    "plt.xlabel('Focal Length Ranges (35mm-equiv)')\n",
    "plt.ylabel('Mean Percentage Error (%)')\n",
    "plt.title('Mean Percentage Error for Different Focal Length Ranges')\n",
    "plt.axhline(y=0, color='black', linewidth=0.5)  # Add a horizontal line at y=0 for reference\n",
    "plt.show()\n",
    "\n",
    "# Calculate mean errors for f-stops\n",
    "mean_errors_fstop = [mean_error_in_range(real_fstops_mape, predicted_fstops_mape, min_val, max_val) for min_val, max_val in fstop_ranges]\n",
    "\n",
    "# Plot mean errors for f-stops\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(range(len(fstop_ranges)), mean_errors_fstop, tick_label=[f\"f/{min_val}-f/{max_val}\" for min_val, max_val in fstop_ranges])\n",
    "plt.xlabel('f-stop Ranges')\n",
    "plt.ylabel('Mean Percentage Error (%)')\n",
    "plt.title('Mean Percentage Error for Different f-stop Ranges')\n",
    "plt.axhline(y=0, color='black', linewidth=0.5)  # Add a horizontal line at y=0 for reference\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compara os modelos de diferentes tamanhos treinados previamente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Define the available ResNet versions\n",
    "resnet_versions = [18, 34, 50, 101, 152]\n",
    "\n",
    "# Define different markers for each ResNet version\n",
    "markers = ['o', 'x', '^', 'D', 'v']\n",
    "\n",
    "# Dictionary to store metadata for each model\n",
    "model_metadata = {}\n",
    "\n",
    "# Function to load metadata for a given model\n",
    "def load_metadata(checkpoint_dir):\n",
    "    training_metadata_file = os.path.join(checkpoint_dir, 'training_metadata.pkl')\n",
    "    testing_metadata_file = os.path.join(checkpoint_dir, 'testing_metadata.pkl')\n",
    "\n",
    "    training_metadata = None\n",
    "    testing_metadata = None\n",
    "\n",
    "    if os.path.exists(training_metadata_file):\n",
    "        with open(training_metadata_file, 'rb') as f:\n",
    "            training_metadata = pickle.load(f)\n",
    "\n",
    "    if os.path.exists(testing_metadata_file):\n",
    "        with open(testing_metadata_file, 'rb') as f:\n",
    "            testing_metadata = pickle.load(f)\n",
    "\n",
    "    return training_metadata, testing_metadata\n",
    "\n",
    "# Load metadata for each available model\n",
    "for version in resnet_versions:\n",
    "    checkpoint_dir = f\"D:\\\\\\\\TEMTC-CN\\\\\\\\checkpoints\\\\\\\\resnet{version}\"\n",
    "    training_metadata, testing_metadata = load_metadata(checkpoint_dir)\n",
    "    if training_metadata and testing_metadata:\n",
    "        model_metadata[version] = {'training': training_metadata, 'testing': testing_metadata}\n",
    "\n",
    "# Check if any metadata was loaded\n",
    "if not model_metadata:\n",
    "    print(\"No trained models found.\")\n",
    "else:\n",
    "    # Extract the data for plotting\n",
    "    epochs = {}\n",
    "    train_losses = {}\n",
    "    val_losses = {}\n",
    "    test_losses = {}\n",
    "    train_mape = {}\n",
    "    val_mape = {}\n",
    "    test_mape = {}\n",
    "    train_msle = {}\n",
    "    val_msle = {}\n",
    "    test_msle = {}\n",
    "    train_mse = {}\n",
    "    val_mse = {}\n",
    "    test_mse = {}\n",
    "\n",
    "    for version, metadata in model_metadata.items():\n",
    "        epochs[version] = metadata['training']['epochs']\n",
    "        train_losses[version] = metadata['training']['train_losses']\n",
    "        val_losses[version] = metadata['training']['val_losses']\n",
    "        test_losses[version] = metadata['testing']['test_losses']\n",
    "        train_mape[version] = metadata['training']['train_mape']\n",
    "        val_mape[version] = metadata['training']['val_mape']\n",
    "        test_mape[version] = metadata['testing']['test_mape']\n",
    "        train_msle[version] = metadata['training']['train_msle']\n",
    "        val_msle[version] = metadata['training']['val_msle']\n",
    "        test_msle[version] = metadata['testing']['test_msle']\n",
    "        train_mse[version] = metadata['training']['train_mse']\n",
    "        val_mse[version] = metadata['training']['val_mse']\n",
    "        test_mse[version] = metadata['testing']['test_mse']\n",
    "\n",
    "        # Apply the workaround to fix the bug of the last epoch not being saved in the testing metadata\n",
    "        epochs[version] = epochs[version][:len(test_losses[version])]\n",
    "        train_losses[version] = train_losses[version][:len(test_losses[version])]\n",
    "        val_losses[version] = val_losses[version][:len(test_losses[version])]\n",
    "        train_mape[version] = train_mape[version][:len(test_losses[version])]\n",
    "        val_mape[version] = val_mape[version][:len(test_losses[version])]\n",
    "        train_msle[version] = train_msle[version][:len(test_losses[version])]\n",
    "        val_msle[version] = val_msle[version][:len(test_losses[version])]\n",
    "        train_mse[version] = train_mse[version][:len(test_losses[version])]\n",
    "        val_mse[version] = val_mse[version][:len(test_losses[version])]\n",
    "\n",
    "    # Plot the comparative graphs\n",
    "    plt.figure(figsize=(24, 24))\n",
    "\n",
    "    # Training Loss Plot\n",
    "    plt.subplot(3, 2, 1)\n",
    "    for i, version in enumerate(resnet_versions):\n",
    "        if version in model_metadata:\n",
    "            plt.plot(epochs[version], train_losses[version], marker=markers[i], label=f'ResNet{version} Treino', linestyle='-')\n",
    "            plt.plot(epochs[version], val_losses[version], marker=markers[i], label=f'ResNet{version} Validação', linestyle='--')\n",
    "    plt.xlabel('Epochs', fontsize=20)\n",
    "    plt.ylabel('Loss', fontsize=20)\n",
    "    plt.title('Loss per epoch - training and validation', fontsize=24)\n",
    "    plt.legend(fontsize=18)\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Test Loss Plot\n",
    "    plt.subplot(3, 2, 2)\n",
    "    for i, version in enumerate(resnet_versions):\n",
    "        if version in model_metadata:\n",
    "            plt.plot(epochs[version], test_losses[version], marker=markers[i], label=f'ResNet{version}')\n",
    "    plt.xlabel('Epochs', fontsize=20)\n",
    "    plt.ylabel('Test Loss', fontsize=20)\n",
    "    plt.title('Test Loss per epoch', fontsize=24)\n",
    "    plt.legend(fontsize=18)\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Training MAPE Plot\n",
    "    plt.subplot(3, 2, 3)\n",
    "    for i, version in enumerate(resnet_versions):\n",
    "        if version in model_metadata:\n",
    "            plt.plot(epochs[version], train_mape[version], marker=markers[i], label=f'ResNet{version} Treino', linestyle='-')\n",
    "            plt.plot(epochs[version], val_mape[version], marker=markers[i], label=f'ResNet{version} Validação', linestyle='--')\n",
    "    plt.xlabel('Epochs', fontsize=20)\n",
    "    plt.ylabel('MAPE (%)', fontsize=20)\n",
    "    plt.title('MAPE per epoch - training and validation', fontsize=24)\n",
    "    plt.legend(fontsize=18)\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Test MAPE Plot\n",
    "    plt.subplot(3, 2, 4)\n",
    "    for i, version in enumerate(resnet_versions):\n",
    "        if version in model_metadata:\n",
    "            plt.plot(epochs[version], test_mape[version], marker=markers[i], label=f'ResNet{version}')\n",
    "    plt.xlabel('Epochs', fontsize=20)\n",
    "    plt.ylabel('Test MAPE (%)', fontsize=20)\n",
    "    plt.title('Test MAPE per epoch', fontsize=24)\n",
    "    plt.legend(fontsize=18)\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Training MSLE Plot\n",
    "    plt.subplot(3, 2, 5)\n",
    "    for i, version in enumerate(resnet_versions):\n",
    "        if version in model_metadata:\n",
    "            plt.plot(epochs[version], train_msle[version], marker=markers[i], label=f'ResNet{version} Treino', linestyle='-')\n",
    "            plt.plot(epochs[version], val_msle[version], marker=markers[i], label=f'ResNet{version} Validação', linestyle='--')\n",
    "    plt.xlabel('Epochs', fontsize=20)\n",
    "    plt.ylabel('MSLE', fontsize=20)\n",
    "    plt.title('MSLE per epoch - training and validation', fontsize=24)\n",
    "    plt.legend(fontsize=18)\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Test MSLE Plot\n",
    "    plt.subplot(3, 2, 6)\n",
    "    for i, version in enumerate(resnet_versions):\n",
    "        if version in model_metadata:\n",
    "            plt.plot(epochs[version], test_msle[version], marker=markers[i], label=f'ResNet{version}')\n",
    "    plt.xlabel('Epochs', fontsize=20)\n",
    "    plt.ylabel('Test MSLE', fontsize=20)\n",
    "    plt.title('Test MSLE per epoch', fontsize=24)\n",
    "    plt.legend(fontsize=18)\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Additional plots for MSE\n",
    "    plt.figure(figsize=(24, 12))\n",
    "\n",
    "    # Training MSE Plot\n",
    "    plt.subplot(1, 2, 1)\n",
    "    for i, version in enumerate(resnet_versions):\n",
    "        if version in model_metadata:\n",
    "            plt.plot(epochs[version], train_mse[version], marker=markers[i], label=f'ResNet{version} Treino', linestyle='-')\n",
    "            plt.plot(epochs[version], val_mse[version], marker=markers[i], label=f'ResNet{version} Validação', linestyle='--')\n",
    "    plt.xlabel('Epochs', fontsize=20)\n",
    "    plt.ylabel('MSE', fontsize=20)\n",
    "    plt.title('MSE per epoch - training and validation', fontsize=24)\n",
    "    plt.legend(fontsize=18)\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Test MSE Plot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    for i, version in enumerate(resnet_versions):\n",
    "        if version in model_metadata:\n",
    "            plt.plot(epochs[version], test_mse[version], marker=markers[i], label=f'ResNet{version}')\n",
    "    plt.xlabel('Epochs', fontsize=20)\n",
    "    plt.ylabel('Test MSE', fontsize=20)\n",
    "    plt.title('Test MSE per epoch', fontsize=24)\n",
    "    plt.legend(fontsize=18)\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teste de (in)sanidade: verifica desempenho inicial do modelo com apenas ImageNet e sem treinamento algum:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import transforms\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# Load the split indices\n",
    "with open(split_indices_path, 'rb') as f:\n",
    "    split_indices = pickle.load(f)\n",
    "\n",
    "dataset = TensorDataset(tensor_directory)\n",
    "test_indices = split_indices['test']\n",
    "test_dataset = Subset(dataset, test_indices)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=0)\n",
    "\n",
    "# Function to evaluate the model and return individual errors\n",
    "def evaluate_model(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    all_real_metadata = []\n",
    "    all_inferred_metadata = []\n",
    "    focal_length_errors = []\n",
    "    fstop_errors = []\n",
    "    all_images = []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images = images.cuda().half()\n",
    "            labels = labels.cuda().float()\n",
    "            with torch.amp.autocast(device_type='cuda'):\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels[:, :2])  # Only consider the first two columns for focal length and f-stop\n",
    "                test_loss += loss.item()\n",
    "                all_real_metadata.extend(labels.cpu().numpy())\n",
    "                all_inferred_metadata.extend(outputs.cpu().numpy())\n",
    "                focal_length_errors.extend(calculate_percentage_error(labels[:, 0].cpu().numpy(), outputs[:, 0].cpu().numpy()))\n",
    "                fstop_errors.extend(calculate_percentage_error(labels[:, 1].cpu().numpy(), outputs[:, 1].cpu().numpy()))\n",
    "                all_images.extend(images.cpu())\n",
    "    test_loss /= len(test_loader)\n",
    "    print(f'Test Loss: {test_loss:.5f}')\n",
    "\n",
    "    all_real_metadata = np.array(all_real_metadata)\n",
    "    all_inferred_metadata = np.array(all_inferred_metadata)\n",
    "    focal_length_errors = np.array(focal_length_errors)\n",
    "    fstop_errors = np.array(fstop_errors)\n",
    "\n",
    "    avg_focal_length_error = np.mean(focal_length_errors)\n",
    "    avg_fstop_error = np.mean(fstop_errors)\n",
    "\n",
    "    print(f'Erro percentual médio - FL: {avg_focal_length_error:.2f}%')\n",
    "    print(f'Erro percentual médio - f/stop: {avg_fstop_error:.2f}%')\n",
    "\n",
    "    return all_images, all_real_metadata, all_inferred_metadata, focal_length_errors, fstop_errors\n",
    "\n",
    "# Evaluate the pre-finetune model (ImageNet weights but not fine-tuned)\n",
    "print(\"\\nResNet{}: Avaliando modelo apenas com pesos ImageNet (pré-finetune)\".format(resnet_version))\n",
    "model_prefinetune = HighResNetRegressor().cuda()\n",
    "criterion = nn.L1Loss()\n",
    "all_images_prefinetune, all_real_metadata_prefinetune, all_inferred_metadata_prefinetune, focal_length_errors_prefinetune, fstop_errors_prefinetune = evaluate_model(model_prefinetune, test_loader, criterion)\n",
    "\n",
    "# Unload the pre-finetune model from VRAM\n",
    "del model_prefinetune\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "# Evaluate the fully untrained model (random weights)\n",
    "print(\"\\nResNet{}: Avaliando modelo completamente não treinado (pesos aleatórios)\".format(resnet_version))\n",
    "selected_resnet_model, _ = resnet_models[resnet_version]\n",
    "selected_resnet = selected_resnet_model(weights=None)  # No weights, random initialization\n",
    "model_untrained = HighResNetRegressor().cuda()\n",
    "all_images_untrained, all_real_metadata_untrained, all_inferred_metadata_untrained, focal_length_errors_untrained, fstop_errors_untrained = evaluate_model(model_untrained, test_loader, criterion)\n",
    "\n",
    "# Unload the untrained model from VRAM\n",
    "del model_untrained\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "# Visualize the best and worst results for the pre-finetune model\n",
    "combined_errors_prefinetune = focal_length_errors_prefinetune + fstop_errors_prefinetune\n",
    "sorted_indices_prefinetune = np.argsort(combined_errors_prefinetune)\n",
    "best_indices_prefinetune = sorted_indices_prefinetune[:5]\n",
    "worst_indices_prefinetune = sorted_indices_prefinetune[-5:]\n",
    "print(\"\\nMelhores resultados pré-finetune:\")\n",
    "visualize_results([all_images_prefinetune[i] for i in best_indices_prefinetune], all_real_metadata_prefinetune[best_indices_prefinetune], all_inferred_metadata_prefinetune[best_indices_prefinetune], num_samples=5)\n",
    "print(\"\\nPiores resultados pré-finetune:\")\n",
    "visualize_results([all_images_prefinetune[i] for i in worst_indices_prefinetune], all_real_metadata_prefinetune[worst_indices_prefinetune], all_inferred_metadata_prefinetune[worst_indices_prefinetune], num_samples=5)\n",
    "\n",
    "\n",
    "# Visualize the best and worst results for the fully untrained model\n",
    "combined_errors_untrained = focal_length_errors_untrained + fstop_errors_untrained\n",
    "sorted_indices_untrained = np.argsort(combined_errors_untrained)\n",
    "best_indices_untrained = sorted_indices_untrained[:5]\n",
    "worst_indices_untrained = sorted_indices_untrained[-5:]\n",
    "print(\"\\nMelhores resultados não treinados:\")\n",
    "visualize_results([all_images_untrained[i] for i in best_indices_untrained], all_real_metadata_untrained[best_indices_untrained], all_inferred_metadata_untrained[best_indices_untrained], num_samples=5)\n",
    "print(\"\\nPiores resultados não treinados:\")\n",
    "visualize_results([all_images_untrained[i] for i in worst_indices_untrained], all_real_metadata_untrained[worst_indices_untrained], all_inferred_metadata_untrained[worst_indices_untrained], num_samples=5)\n",
    "\n",
    "\n",
    "# Plot error trends with real 35mm-equivalent values for pre-finetune model\n",
    "plt.figure(figsize=(18, 6))\n",
    "\n",
    "# Plot focal length error vs true focal length\n",
    "plt.subplot(1, 2, 1)\n",
    "real_focal_lengths_prefinetune = [denormalize_focal_length(val, focal_lenght_min, focal_lenght_max) for val in all_real_metadata_prefinetune[:, 0]]\n",
    "plt.scatter(real_focal_lengths_prefinetune, focal_length_errors_prefinetune, alpha=0.5)\n",
    "plt.xscale('log')\n",
    "plt.xlabel('True Focal Length (35mm-equiv)')\n",
    "plt.ylabel('Percentage Error (%)')\n",
    "plt.title('Focal Length Error Trend (pré-finetune)')\n",
    "plt.xticks([16, 20, 28, 35, 50, 70, 85, 105, 135, 200, 250, 300, 400, 500, 600], [f\"{x}\" for x in [16, 20, 28, 35, 50, 70, 85, 105, 135, 200, 250, 300, 400, 500, 600]])\n",
    "\n",
    "# Plot f-stop error vs true f-stop\n",
    "plt.subplot(1, 2, 2)\n",
    "real_fstops_prefinetune = [denormalize_fstop(val, fstop_min, fstop_max) for val in all_real_metadata_prefinetune[:, 1]]\n",
    "plt.scatter(real_fstops_prefinetune, fstop_errors_prefinetune, alpha=0.5)\n",
    "plt.xscale('log')\n",
    "plt.xlabel('True f-stop')\n",
    "plt.ylabel('Percentage Error (%)')\n",
    "plt.title('f-stop Error Trend (pré-finetune)')\n",
    "plt.xticks([2, 2.8, 4, 5.6, 8, 11, 16, 22, 32], [f\"f/{f}\" for f in [2, 2.8, 4, 5.6, 8, 11, 16, 22, 32]])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot error trends with real 35mm-equivalent values for fully untrained model\n",
    "plt.figure(figsize=(18, 6))\n",
    "\n",
    "# Plot focal length error vs true focal length\n",
    "plt.subplot(1, 2, 1)\n",
    "real_focal_lengths_untrained = [denormalize_focal_length(val, focal_lenght_min, focal_lenght_max) for val in all_real_metadata_untrained[:, 0]]\n",
    "plt.scatter(real_focal_lengths_untrained, focal_length_errors_untrained, alpha=0.5)\n",
    "plt.xscale('log')\n",
    "plt.xlabel('True Focal Length (35mm-equiv)')\n",
    "plt.ylabel('Percentage Error (%)')\n",
    "plt.title('Focal Length Error Trend (não treinado)')\n",
    "plt.xticks([16, 20, 28, 35, 50, 70, 85, 105, 135, 200, 250, 300, 400, 500, 600], [f\"{x}\" for x in [16, 20, 28, 35, 50, 70, 85, 105, 135, 200, 250, 300, 400, 500, 600]])\n",
    "\n",
    "# Plot f-stop error vs true f-stop\n",
    "plt.subplot(1, 2, 2)\n",
    "real_fstops_untrained = [denormalize_fstop(val, fstop_min, fstop_max) for val in all_real_metadata_untrained[:, 1]]\n",
    "plt.scatter(real_fstops_untrained, fstop_errors_untrained, alpha=0.5)\n",
    "plt.xscale('log')\n",
    "plt.xlabel('True f-stop')\n",
    "plt.ylabel('Percentage Error (%)')\n",
    "plt.title('f-stop Error Trend (não treinado)')\n",
    "plt.xticks([2, 2.8, 4, 5.6, 8, 11, 16, 22, 32], [f\"f/{f}\" for f in [2, 2.8, 4, 5.6, 8, 11, 16, 22, 32]])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate residuals for pre-finetune model\n",
    "predicted_focal_lengths_prefinetune = [denormalize_focal_length(val, focal_lenght_min, focal_lenght_max) for val in all_inferred_metadata_prefinetune[:, 0]]\n",
    "residuals_fl_prefinetune = np.array(real_focal_lengths_prefinetune) - np.array(predicted_focal_lengths_prefinetune)\n",
    "\n",
    "predicted_fstops_prefinetune = [denormalize_fstop(val, fstop_min, fstop_max) for val in all_inferred_metadata_prefinetune[:, 1]]\n",
    "residuals_fstop_prefinetune = np.array(real_fstops_prefinetune) - np.array(predicted_fstops_prefinetune)\n",
    "\n",
    "# Plot residuals vs actual values for pre-finetune model\n",
    "plt.figure(figsize=(18, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(real_focal_lengths_prefinetune, residuals_fl_prefinetune, alpha=0.5)\n",
    "plt.xlabel('True Focal Length (35mm-equiv)')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residuals vs True Focal Length (pré-finetune)')\n",
    "plt.xscale('log')\n",
    "plt.xticks([16, 20, 28, 35, 50, 70, 85, 105, 135, 200, 250, 300, 400, 500, 600], [f\"{x}\" for x in [16, 20, 28, 35, 50, 70, 85, 105, 135, 200, 250, 300, 400, 500, 600]])\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(real_fstops_prefinetune, residuals_fstop_prefinetune, alpha=0.5)\n",
    "plt.xlabel('True f-stop')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residuals vs True f-stop (pré-finetune)')\n",
    "plt.xscale('log')\n",
    "plt.xticks([2, 2.8, 4, 5.6, 8, 11, 16, 22, 32], [f\"f/{f}\" for f in [2, 2.8, 4, 5.6, 8, 11, 16, 22, 32]])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate residuals for fully untrained model\n",
    "predicted_focal_lengths_untrained = [denormalize_focal_length(val, focal_lenght_min, focal_lenght_max) for val in all_inferred_metadata_untrained[:, 0]]\n",
    "residuals_fl_untrained = np.array(real_focal_lengths_untrained) - np.array(predicted_focal_lengths_untrained)\n",
    "\n",
    "predicted_fstops_untrained = [denormalize_fstop(val, fstop_min, fstop_max) for val in all_inferred_metadata_untrained[:, 1]]\n",
    "residuals_fstop_untrained = np.array(real_fstops_untrained) - np.array(predicted_fstops_untrained)\n",
    "\n",
    "# Plot residuals vs actual values for fully untrained model\n",
    "plt.figure(figsize=(18, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(real_focal_lengths_untrained, residuals_fl_untrained, alpha=0.5)\n",
    "plt.xlabel('True Focal Length (35mm-equiv)')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residuals vs True Focal Length (não treinado)')\n",
    "plt.xscale('log')\n",
    "plt.xticks([16, 20, 28, 35, 50, 70, 85, 105, 135, 200, 250, 300, 400, 500, 600], [f\"{x}\" for x in [16, 20, 28, 35, 50, 70, 85, 105, 135, 200, 250, 300, 400, 500, 600]])\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(real_fstops_untrained, residuals_fstop_untrained, alpha=0.5)\n",
    "plt.xlabel('True f-stop')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residuals vs True f-stop (não treinado)')\n",
    "plt.xscale('log')\n",
    "plt.xticks([2, 2.8, 4, 5.6, 8, 11, 16, 22, 32], [f\"f/{f}\" for f in [2, 2.8, 4, 5.6, 8, 11, 16, 22, 32]])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Bias Analysis: Mean error for different ranges\n",
    "def mean_error_in_range(real_values, predicted_values, min_val, max_val):\n",
    "    real_values = np.array(real_values)\n",
    "    predicted_values = np.array(predicted_values)\n",
    "    mask = (real_values >= min_val) & (real_values < max_val)\n",
    "    errors = predicted_values[mask] - real_values[mask] \n",
    "    return np.mean(errors)\n",
    "\n",
    "# Define ranges for focal lengths and f-stops\n",
    "fl_ranges = [(16, 20), (20, 35), (35, 50), (50, 70), (70, 85), (85, 105), (105, 135), (135, 200), (200, 250), (250, 300), (300, 400), (400, 500), (500, 600)]\n",
    "fstop_ranges = [(2, 2.8), (2.8, 4), (4, 5.6), (5.6, 8), (8, 11), (11, 16), (16, 22), (22, 32)]\n",
    "\n",
    "# Calculate mean errors for focal lengths\n",
    "mean_errors_fl = [mean_error_in_range(real_focal_lengths_mape, predicted_focal_lengths_mape, min_val, max_val) for min_val, max_val in fl_ranges]\n",
    "\n",
    "# Bias Analysis: Mean error for different ranges for pre-finetune model\n",
    "mean_errors_fl_prefinetune = [mean_error_in_range(real_focal_lengths_prefinetune, predicted_focal_lengths_prefinetune, min_val, max_val) for min_val, max_val in fl_ranges]\n",
    "mean_errors_fstop_prefinetune = [mean_error_in_range(real_fstops_prefinetune, predicted_fstops_prefinetune, min_val, max_val) for min_val, max_val in fstop_ranges]\n",
    "\n",
    "# Plot mean errors for focal lengths for pre-finetune model\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(range(len(fl_ranges)), mean_errors_fl_prefinetune, tick_label=[f\"{min_val}-{max_val}\" for min_val, max_val in fl_ranges])\n",
    "plt.xlabel('Focal Length Ranges (35mm-equiv)')\n",
    "plt.ylabel('Mean Error')\n",
    "plt.title('Mean Error for Different Focal Length Ranges (pré-finetune)')\n",
    "plt.axhline(y=0, color='black', linewidth=0.5)  # Add a horizontal line at y=0 for reference\n",
    "plt.show()\n",
    "\n",
    "# Plot mean errors for f-stops for pre-finetune model\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(range(len(fstop_ranges)), mean_errors_fstop_prefinetune, tick_label=[f\"f/{min_val}-f/{max_val}\" for min_val, max_val in fstop_ranges])\n",
    "plt.xlabel('f-stop Ranges')\n",
    "plt.ylabel('Mean Error')\n",
    "plt.title('Mean Error for Different f-stop Ranges (pré-finetune)')\n",
    "plt.show()\n",
    "\n",
    "# Bias Analysis: Mean error for different ranges for fully untrained model\n",
    "mean_errors_fl_untrained = [mean_error_in_range(real_focal_lengths_untrained, predicted_focal_lengths_untrained, min_val, max_val) for min_val, max_val in fl_ranges]\n",
    "mean_errors_fstop_untrained = [mean_error_in_range(real_fstops_untrained, predicted_fstops_untrained, min_val, max_val) for min_val, max_val in fstop_ranges]\n",
    "\n",
    "# Plot mean errors for focal lengths for fully untrained model\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(range(len(fl_ranges)), mean_errors_fl_untrained, tick_label=[f\"{min_val}-{max_val}\" for min_val, max_val in fl_ranges])\n",
    "plt.xlabel('Focal Length Ranges (35mm-equiv)')\n",
    "plt.ylabel('Mean Error')\n",
    "plt.title('Mean Error for Different Focal Length Ranges (não treinado)')\n",
    "plt.axhline(y=0, color='black', linewidth=0.5)  # Add a horizontal line at y=0 for reference\n",
    "plt.show()\n",
    "\n",
    "# Plot mean errors for f-stops for fully untrained model\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(range(len(fstop_ranges)), mean_errors_fstop_untrained, tick_label=[f\"f/{min_val}-f/{max_val}\" for min_val, max_val in fstop_ranges])\n",
    "plt.xlabel('f-stop Ranges')\n",
    "plt.ylabel('Mean Error')\n",
    "plt.title('Mean Error for Different f-stop Ranges (não treinado)')\n",
    "plt.show()\n",
    "\n",
    "print(\"Modelos pré-finetune e não treinados descarregados da VRAM.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
